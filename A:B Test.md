# A/B Test

## 假设检验

<img src="../../data-analyst-document/image/A:B-01.png" alt="image-20230320173523111" style="zoom:67%;" />



### A/B 测试中的假设检验

A/B 测试的语境中，假设⼀般是指**关于实验组和对照组指标的⼤⼩的推断。** 

在假设检验中的“假设”是⼀对:零假设(Null Hypothesis)和备择假设(Alternative Hypothesis)，它们是完全相反的。

在 A/B 测试的语境下，零假设指的是实验组和对照组的指标是相同的，备择假设指的是实验组和对照组的指标是不同的。



---

### 单尾、双尾检验

单尾检验(One-tailed Test)和双尾检验(Two-tailed Test)这两个概念。

- 单尾检验⼜叫单边检验(One-sided Test)，它不仅在假设中说明了两个⽐较对象不同，并且还明确了谁⼤谁⼩，⽐如实验组的指标⽐对照组的指标⼤。

- 双尾检验⼜叫双边检验(Two-sided Test)，指的是仅仅在假设中说明了两个⽐较对象不同，但是并没有明确谁⼤谁⼩。

> 单边检验可能需要对数据有主观意识的代⼊，例如指标应该更⼤，或者更⼩，⽽双边检验不带有这种主观想法，只是单纯利⽤数据来判断相同还是不同。



#### 在 A/B 测试的实践中，更推荐使⽤双尾检验。

**原因如下：**

- 第⼀个原因是，双尾检验可以让数据⾃⾝在决策中发挥更⼤的作⽤。

  > 我们在实践中使⽤ A/B 测试，就是希望能够通过数据来驱动决策。**我们要尽量减少在使⽤数据前产⽣的任何主观想法来⼲扰数据发挥作⽤。** 
  >
  > 所以，双尾检验这种不需要我们明确谁⼤谁⼩的检验，更能发挥数据的作⽤。

- 第⼆个原因：双尾检验可以帮助全⾯考虑变化带来的正、负⾯结果。

  > 双尾检验可以同时照顾到正⾯和负⾯的结果，更接近多变的现实情况。但是单尾检验只会适⽤于其中⼀种，⽽且通常是我们期望的正⾯效果。



---

### 其他检验

检验有很多种，单尾检验和双尾检验，是从“假设”的⾓度来分类的。除此之外， 常⻅的“检验”还可以根据⽐较样本的个数进⾏分类，包括**单样本检验(One-Sample Test)、 双样本检验(Two-Sample Test)和配对检验(Paired Test)**



#### 各个检验的使⽤范围

- 当两组样本数据进⾏⽐较时，就⽤**双样本检验**

  > ⽐如 A/B 测试中实验组和对照组的⽐较。

- 当⼀组样本数据和⼀个具体数值进⾏⽐较时，就⽤**单样本检验**

  > ⽐如，我想⽐较极客时间⽤户的⽇均使⽤时间有没有达到 15 分钟，这个时候，我就可以把⼀组样本数据(抽样所得的极客时间⽤户的每⽇使⽤时间)和⼀个具体数值$15$来进⾏⽐较。

- 当⽐较同⼀组样本数据发⽣变化前和发⽣变化后时，就⽤**配对检验** 

  > ⽐如，我现在随机抽取 1000 个极客时间的⽤户，给他们“全场专栏⼀律 1 折”这个优惠，然后在这1000 个⼈中，我们会⽐较他们在收到优惠前⼀个⽉的⽇均使⽤时间，和收到优惠后⼀个⽉的⽇均使⽤时间。



> **在 A/B 测试中，使⽤双样本检验。**



---

### T检验、Z检验

主要看样本量的⼤⼩和是否知道**总体⽅差(Population Variance)**:

- 当我们不知道总体⽅差时，使⽤ T 检验
- 当我们已知总体⽅差，且样本量⼤于 30 时，使⽤ Z 检验



<img src="../../data-analyst-document/image/A:B-02.png" alt="image-20230320191254900" style="zoom:30%;" />



> 在统计中我们习惯说样本量⼤于30 就是很⼤的样本，就可以⽤样本⽅差来近似总体⽅差，这样我们就知道总体⽅差，就可以⽤Z检验了，但其实30只是经验值，⼤于30的总体⽅差也是样本⽅差近似的，所以如果准确的说的话样本量⼤于30，在总体⽅差未知的情况下，也要⽤T检验。



这些理论具体到 A/B 测试实践中，⼀个经验就是：**均值类指标⼀般⽤ T 检验，概率类指标⼀般⽤ Z 检验(⽐例检验)。**



> 在样本量⾜够⼤的情况下 T 分布近似于Z分布，所以如果你不知道该⽤t检验还是z检验，⽽**样本量够⼤时，直接⽤T分布即可**; 



> ⽐例检验(Proportion Test) 是，专指⽤于**检验概率类指标的 Z 检验**。 

---



### 利⽤假设检验做出判断

假设检验会推断出两种结果:

1. 接受零假设，拒绝备择假设，也就是说实验组和对照组的指标是相同的

2. 接受备择假设，拒绝零假设，也就是说实验组和对照组的指标是不同的

|                      | 假设检验两组指标不同 | 假设检验两组指标相同 |
| :------------------: | :------------------: | :------------------: |
| 假设检验两组指标不同 |       推断正确       |      第一类错误      |
| 假设检验两组指标相同 |      第二类错误      |       推断正确       |



#### 第⼀类错误(Type I Error)

**定义：**统计上的定义是拒绝了事实上是正确的零假设 

> 在 A/B 测试中，零假设是两组的指标是相同的，当假设检验推断出两组指标不同，但事实上两组指标相同时，就是第⼀类错误。我们把两组指标不同称作阳性(Positive)。所以，第⼀类错误⼜叫假阳性(False Positive)。



> 发⽣第⼀类错误的概率⽤α表⽰，也被称为**显著⽔平(Significance Level)**。“显著”是指错误发⽣的概率⼤，统计上把发⽣率⼩于 5% 的事件称为⼩概率事件，代表这类事件不容易发⽣。因此显著⽔平⼀般也为 5%。 



#### 第⼆类错误(Type II Error)

**定义：**统计上的定义是接受了事实上是错误的零假设。

> 在 A/B 测试中，当假设检验推断出两组指标相同，但事实上两组指标是不同时，就是第⼆类错误。
>
> 我们把两组指标相同称作阴性(Negative)，所以第⼆类错误⼜叫假阴性(FalseNegative)。发⽣第⼆类错误的概率⽤β表⽰，统计上⼀般定义为 20%。



---

### 得出测试结果

实践中常⽤的有两种⽅法:

- P值法(P-Value)
- 置信区间法(Confidence Interval)



#### P值法

**定义：**在统计上，P 值就是**当零假设成⽴**时，我们所观测到的样本数据出现的概率 

> 在 A/B 测试 的语境下，P 值就是当对照组和实验组指标事实上是相同时，在 A/B 测试中⽤样本数据所观测到的“实验组和对照组指标不同”出现的概率。



> 假设零假设是正确的，我们通过真实数据样本观测到零假设事件发⽣的概率。这个发⽣概率很低，说明通过样本数据，其实零假设事件发⽣是个很低概率事件，甚⾄低于我们最⼩可以接受的概率值(显著性⽔平），应该按照⼩概率事件不可能发⽣原理，零假设事件不会发⽣，即拒绝零假设。
>
> 与此相反的是，当我们在 A/B 测试中观测到“实验组和对照组指标不同”的概率(P 值) 很⼤，⽐如 70%，那么在零假设成⽴时，我们观测到这个事件还是很有可能的。所以这个时候我们接受零假设，拒绝备择假设，即两组指标是相同的。



在统计中，我们会⽤ P 值和显著⽔平α进⾏⽐较，⼜因为α⼀般取 5%，所以就⽤ P 值和5% 进⾏⽐较，就可以得出假设检验的结果了:

- 当P值⼩于5%时，我们拒绝零假设，接受备择假设，得出两组指标是不同的结论，⼜叫做结果显著
- 当P值⼤于5%时，我们接受零假设，拒绝备择假设，得出两组指标是相同的结论，⼜叫做结果不显著



**如何计算P值：**

- ⽐例检验，可以⽤ Python 的 **proportions_ztest** 函数、R 的 **prop.test** 函数
- T 检验，可以⽤ Python的 **ttest_ind** 函数、R的 **t.test** 函数



> 需要注意：当需要计算假阳率时，需要同时满⾜$p<0.05$且零假设为真的条件，两个条件要同时发⽣



#### 置信区间法

置信区间是⼀个范围，⼀般前⾯会跟着⼀个百分数，最常⻅的是 95% 的置信区间。这是什么意思呢?在统计上，对于⼀个随机变量来说，有 95% 的概率包含总体平均值(Population mean)的范围，就叫做 95% 的置信区间。

置信区间的统计定义其实不是特别好懂，其实你可以直接把它理解为随机变量的波动范围，95% 的置信区间就是包含了整个波动范围的 95% 的区间。

> 置信⽔平表⽰置信区间包含真正的实验效应的频率（100次有多少次）



**A/B 测试本质上就是要判断对照组和实验组的指标是否相等，那怎么判断呢?**

答案就是计算实验组和对照组指标的差值$δ$。因为指标是随机变量，所以它们的差值$δ$也会是随机变量，具有⼀定的波动性。

这就意味着，我们就要计算出$δ$的置信区间，然后看看这个置信区间是否包括 0。

- 如果包括 0 的话，则说明$δ$有可能为 0，意味着两组指标有可能相同
- 如果不包括 0，则说明两组指标不同



> 例如，计算得出两组指标差值$δ$的 95% 置信区间为$ [0.005,0.011]$，不包含 0，也可以推断出两组指标显著不同。
>
> 若实验组和对照组分别的置信区间有95%区域不重叠，则实验效应应该是统计显著的，此时$p值<0.05$.



> $!$**理解95%：**95%表⽰经过许多研究计算得到的95%置信区间，例如进⾏100次研究计算，会得到100个对应的95%置信区间，⽽在这100个95%置信区间中，有多少频率、有⼏个置信区间会包含真正的实验效应。



#### P值与置信区间的对偶性

对于对照试验中常⽤的零差异零假设，实验效应的95%置信区间不包含零，意味着$P值<0.05$



---



### FAQ

> **AB测试是否也能转换成单样本检验?比如AB两组样本，用A样本的均值标准差，和B样本做单样本检验? 通常用excel的Z.TEST时会这么干，会有什么问题吗**
>
> 双样本检验是两个有波动性的随机变量在比较，单样本检验时一个随机变量和个常数比较，你把其中一个变量简化成一个常数肯定会丢失掉原数据的一些特征嘛结果肯定没有双样本检测准确的，所以A/B测试是不推荐单样本检测的。



> **如果不只两个实验可以用t或z检验吗? 一个对照组两个实验组，用实验组分别和对照组做假设检验吗?**
>
> 对的!你说的是A/B/n测试，这里面有不止一个实验组，这是后就要用实验组分别和对照组做假设检验



> **如何检验两个样本比率是否发生变化** 
>
> 用独立性检验也就是卡方检验来验证两个样本比率是否发生变化。



---

## 统计功效

### 定义与理解

统计功效Power，又被称作 Statistical Power：**如果变体之间存在真实差异，检出出这个有意义的差值的概率（统计上指当真实有差异的时候拒绝零假设的概率）**

Power 的本质是概率，在 A/B 测试中，如果实验组和对照组的指标事实上是不同的，Power 指的就是通过 A/B 测试探测到两者不同的概率。

> 在实验组和对照组中事实上确实存在差异时，AB测试准确检测出差异的概率。
>
> Power越大，就越能探测到两组的不同。**把 Power 看成 A/B 测试的灵敏度就可以了** 



$\alpha=P(\text{reject null}\mid \text{null true})$

$\beta=P(\text{fail to reject}\mid \text{null false})$

$1-\beta = \text{sensitivity}$： 通常为80%

> 小样本：
>
> - $\alpha$ 小
> - $\beta$ 高
>
> 大样本：
>
> - $\alpha$ 不变
> - $\beta$ 更小



统计功效是在测试中检测出选件之间转化率真实差异的概率。由于转化事件存在随机性，因此即使两个选件之间的转化率在长期测试中存在实际差异，该测试可能也不会显示具有统计意义的显著差异。可以认为这就是运气不好或纯属偶然。我们将这种未能检测到转化率真实差异的情况称为漏报或 II 类错误。



### 例子

我们先把用户分为对照组和实验组，其中:

- 对照组是正常的用户注册流程，输入个人基本信息一短信/邮箱验证注册成功实验组是，在正常的用户注册流程中，还加入了微信、微博等第三方账号登录的功能用户可以通过第三方账号一键注册登录

- 相信不用我说，你也能猜到，实验组用户的注册率肯定比对照组的要高，因为实验组帮用户省去了繁琐的注册操作。这就说明，在事实上这两组用户的注册率是不同的

那么，现在如果 A/B 测试有 80% 的 Power，就意味着这个 A/B 测试有 80% 的概率可以准确地检测到这两组用户注册率的不同，得出统计显著的结果。换句话说，这个 A/B 测试有 20% 的概率会错误地认为这两组用户的注册率是相同的
可见，Power 越大，说明 A/B 测试越够准确地检测出实验组与对照组的不同(如果两组事实上是不同的)



> 当对照实验的置信区间包含0，并不意味着置信区间中的零比其他值更有可能出现，实验很可能没有足够的统计功效。



---

## 实验框架

<img src="../../data-analyst-document/image/A:B-02.png" alt="image-20230321142228297" style="zoom:50%;" />



1. **从业务问题出发，确定AB测试的目标和假设**

2. **确定AB测试的评价指标**

   - 确定触发条件

   - 定义实验中的用户

   - 确定用户使用功能的时间窗口期

   - 指标类型：

     - 核心指标：用以监控评估实验效果；

     - 辅助指标：辅助核心指标，用以评估实验效果

     - 不变指标：用以评估实验环境变化；

     - 负面指标：用于观测实验风险；

3. **选取实验对象的单位**

   - 用户ID，IP地址，Cookies ID等

     

4. **明确置信水平、统计功效、想要观测的最小变化量来计算所需的样本大小**

   - 确定统计量：
     - 显著性水平
     - Power
     - 实验组、对照组的综合方差
     - 实验组、对照组的评价指标差值
       - 可以根据经验确定

5. **确定分流策略和实验所需的时间**

   - 确定好样本量之后便可以开始正式分组了。

   - 需要注意多实验开启时，需要正交分流，防止实验之间的交互影响。

6. **分析测试结果**
   - 合理性检验：保证测试的质量、确保AB测试具体实施过程符合预期设计
     - 使用**护栏指标**检验： 
       - 实验组、对照组样本大小比例
       - 实验组、对照组中的特征分布是否相似

7. **正式分析结果**
   - 计算P值、置信区间



---



## AB测试不适用的场景



1. 当没有办法**控制想要测试的变量**时：AB测试是控制变量实验，而控制变量前提是我们能够人为控制，例如一些需要用户自己个人选择决定的变量 
2. 当有**重大事件发布**时：例如新产品、新业务的发布，或者产品形象的变化 
   - 例如产品代言人、公司的商标
3. 当**用户数量很少**时：当流量很少时，很难在短时间内达到所需要的样本量 
4. 不适用对一些初期不成熟想法的验证



### **AB测试不适用时，有哪些替代的方法**

#### 倾向评分匹配(Propensity Score Matching)

1. **非实验的因果推断方法，又叫观察性研究**，这其中最常用的就是倾向评分匹配 (Propensity Score Matching)，简称 PSM。

   > 本质就是在历史数据中，通过模型的方法（例如使用逻辑回归模型，得到每个人的倾向得分，再根据相似的倾向得分匹配），人为地(而不是像实验那样随机地)构建出相似 的实验组和对照组，最后再对两组进行比较。

2. 实例：由于用户是否升级这个变化因素是用户的自主选择，我们并不能控制，所以就并不能做随机分配的实验。那么这个时候，非实验的因果推断方法 PSM 就可以派上用场啦。

   - 根据用户画像和使用行为进行匹配

   - 经过 PSM 处理后的没有升级的用户和升级的用户，在各个特征上都已经非常相似了，那么这个时候我们就可以进行比较了。当我们比较时，因为已经控制了其他特征相似，两组只有“是否升级”这一项不同



#### 用户研究

用户研究适用于 A/B 测试无法进行时，比如新产品 / 业务发布前的测评，我们就可以通过 直接或间接的方式，和用户交流沟通来获取信息，从而判断相应的变化会对用户产生什么 影响。

1. **深度用户体验研究**(Deep User Experience Research)： 通过**选取几个潜在用户进行深度的信息提取**，比如通过用户眼 球的运动来追踪用户的选择过程的眼动研究，或者用户自己记录的日记研究
2. **焦点小组**(Focus Group)：有引导的小组讨论，由主持人把潜在的用户组织起来，引导大家讨论不同的话题，然后根据大家在讨论中发表的不同意见，综合得出反馈意见。
3. **调查问卷**(Survey): 通过事先设计好的问题，选择题或开放性问题，将问题做成问卷发给潜在用户



---

## A/A 实验

A/A实验是指将参与测试的用户随机分配到两个相同的测试组中，然后让这两个测试组分别体验相同的页面或产品，通过比较两组的数据差异来**确定测试结果的可靠性**。A/A实验的主要目的是验证实验设计的有效性和实验过程的准确性，确保实验过程中没有出现误差和偏差，例如测试工具的不准确性或样本的选择偏差等。



**Q：** AA空跑实验是什么？目的是什么？如何评估AA实验是否通过？ 

**A：**AA空跑实验是在AB实验上线前进行的，其原理是在即将上线的AB实验分组中，每个组均不采取任何新策略，验证各个组之间是否存在显著性差异。**确定两组流量是在实验期具有可比性的。**

- 目的：验证各个分组在自然情况下是一致的，**保证AB实验的差异是由新策略所带来的**。



AA实验的数据，可以采用**空跑一段时间**或**回溯过往时间**两种方式，建议采用后者，可以缩短实验周期，提升效率。

AB实验的核心是假设检验，因此评估AA实验是否通过，也是通过假设检验方式(T检验/Z检验)，计算对应的p值，是否拒绝原假设(AA分桶是否有显著差异)。同最小样本量计算方式一样，不同类型指标计算方式存在一定差异。



---



## 实验指标

### 护栏指标与评价指标

A/B 测试的指标分为评价指标(Evaluation Metrics)和护栏指标(Guardrail Metrics)这两类。

#### 评价指标

一般指能驱动公司 / 组织实现核心价值的指标，又被称作驱动指标。

- 评价指标通常是短期的、比较敏感、有很强的可操作性，例如点击率、转化率、人均使用时长等。
- 评价指标需要满足一下特征：
  - **可归因性：**
  - 可测量性： 需要可以被量化
  - 敏感性：指标要能敏感地反映出实验中变量的变化
  - 稳定性：其他因素变化了，指标要能保持相对的稳定



#### 护栏指标 

属于 A/B 测试中基本的合理性检验，就像飞机起飞前的安全检查一样。它的作用就是作为**辅助**，来保障 A/B 测试的质量

- 衡量 A/B 测试是否符合业务上的长期目标，不会因为优化短期指标而打乱长期目标。
- 确保从统计上尽量减少出现各种偏差(Bias)，得到尽可能值得信任的实验结果。

<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321191906184.png" alt="image-20230321191906184" style="zoom:50%;" />

##### 业务品质层面

护栏指标：保证用户体验的同时，兼顾盈利能力和用户参与度。

通常会用到的护栏指标主要是三个:网络延迟(Latency)、闪退率(Crush Rate)和人均指标。

- 网络延迟：**网页加载时间、App 响应时间等，都是表征网络延迟的护栏指标**。增加产品功能可能会增加网页或 App 的响应时间，而且用户可以敏感地察觉出来。
- 闪退率：闪退发生的概率虽然不大，但是会严重影响用户体验。
- 人均指标：
  - **收入角度**，比如人均花费、人均利润等。 → 反应产品的盈利能力
  - **用户参与度**，比如人均使用时长、人均使用频率等。 → 反应用户的满意程度



##### 统计品质层面

统计方面主要是尽可能多地消除偏差，使实验组和对照组尽可能相似，比如检测两组样本量的比例，以及检测两组中特征的分布是否相似。

造成偏差的原因：

- 随机分组的算法出现bug

- 样本量不够大

- 触发实验条件的数据出现延迟等

  

**问题类型**  

1. 实验组、对照组样本大小比例：
   - 实验组和对照组样本大小的比例，预期是 1:1=1。但有的时候，当实验结束后却发现两者的比例并不等于 1，甚至也没有很接近 1。这就说明这个实验在具体实施的过程中出现了问题，导致实验组和对照组出现了偏差。
   
     
   
2. 实验组、对照组中特征的分布
   - A/B 测试中一般采取随机分组，来保证两组实验对象是相似的，从而达到控制其他变量、 只变化我们关心的唯一变量(即 A/B 测试中的原因)的目的。



> 有了评价指标，就可以保证 A/B 测试的成功了吗?显然不是的。很多时候，我们可能考虑得不够全面，忽略了测试本身的合理性，不确定测试是否会对业务有负面效果，因此很可能得出错误的结论。         举个例子。如果为了优化一个网页的点击率，就给网页添加了非常酷炫的动画效果。结果点击率是提升了，网页加载时间却增加了，造成了不好的用户体验。长期来看，这就不利于业务的发展。



### 测量评价指标的敏感性和稳定性

业界通常采用 **A/A 测试来测量稳定性，用回溯性分析来表征敏感性**。

A/A 测试(A/A Test)也是把被测试对象分成实验组和对照组。但不同的是，A/A 测试中两组对象拥有的是完全相同的体验，如果 A/A 测试的结果发现两组的指标有显著不同，那么就说明要么分组分得不均匀，每组的**数据分布差异较大**;要么选取的指标波动范围太大，稳定性差。

如果没有之前实验的数据，或者是因为某些原因(比如时间不够)没有办法跑新的实验，那我们也可以通过分析历史数据，进行**回溯性分析**。也就是在分析之前不同的产品变化时，去看我们感兴趣的指标是否有相应的变化。



### 选取具体的评价指标

1. 清楚业务产品当前所处的阶段，根据阶段的目标，确定评价指标：例如起步阶段、发展阶段、成熟阶段

2. 如果目标较抽象，则采用定性+定量结合的方法：问卷调查、用户调研等定性方法，将定性的调研结果与定量的用户使用行为分析结合

3. 有条件，则可以通过公开或非公开的渠道，参考其他公司相似的实验或研究，根据自身实际情况借鉴他们使用的评价指标

   

当需要综合考虑多个指标时，我们需要综合考虑改动所带来的好处和潜在的损失，结合多个指标，构建一个总体评价标准 (Overall Evaluation Criteria，简称 **OEC**)。 

当要考察的事物包含多个方面时，只有综合各方面的指标，才能把握总体的好坏。这也是使用 OEC 最明显的一个好处。最常见的一类 OEC，就 是亚马逊的这种结合变化带来的潜在收益和损失的 OEC。需要注意的是，这里的“损失”还有可能是护栏指标，也就是说 OEC 有可能会包含护栏指标。



> ✅ 使用 OEC 的另一个好处就是可以避免**多重检验问题(Multiple Testing Problem)**。如果我们不把不同的指标加权结合起来分析，而是单独比较它们，就会出现多重检验的问题，导致 A/B 测试的结果不准确。多重检验问题是 A/B 测试中一个非常常见的误区. 



### 衡量评价指标的波动性

在实践中，计算波动范围一般有统计公式和实践经验两类方法。



#### 统计公式确定

统计公式： $\text{置信区间=样本均值(sample mean)}\pm \text{Z分数}\times \text{标准误差}$

1. 根据中心极限定理，当样本量足够大时，大部分情况下数据服从正态分布，所以这里选用 z 分数。
2. 在一般情况下我们选用 95% 的置信区间，对应的 z 分数为 1.96。



标准误差的计算非常关键，常见对概率类和均值类指标计算。

- 概率类的指标

  ，常见的有用户点击的概率(点击率)、转化的概率(转化率)、购买的概 率(购买率)，等等。

  - $\text{Standard Error} = \sqrt{\frac{p(1-p)}n}$.

- 均值类的指标

  ，常见的有用户的平均使用时长、平均购买金额、平均购买频率，等等。根 据中心极限定理，这类指标通常也是正态分布。

  - $\text{Standard Error}=\sqrt{\frac{s^2}n}=\sqrt{\frac{\sum_i^n(x_i-\bar{x})^2}{n(n-1)}}$.



#### 实践经验确定

根据实践经验确定标准差：

在实际应用中，有些复杂的指标可能不符合正态分布，或者我们根本不知道它们是什么分布，就很难甚至是没办法找到相应的统计公式去计算了。这时候，要得到评价指标的波动范围，我们需要结合实践经验来估算。

- **A/A测试**：**跑多个不同样本大小的 A/A 测试**，然后分别计算每个样本的指标大小，计算出来后，再把这些指标从小到大排列起来，并且去除最小 2.5% 和最大 2.5% 的值，剩下的就是 95% 的置信区间。

- Bootstrapping 算法：

   先跑一个样本很大的 A/A 测试，然后在这个大样本中进行随机可置换抽样 (Random Sample with Replacement)， 抽取不同大小的样本来分别计算指标。然后采用和 A/A 测试一样的流程:把这些指标从小到大排列起来，并且去除最小 2.5% 和最大 2.5% 的值，得到 95% 的置信区间。

  - 在实际应用中，Bootstrapping 会更流行些，因为只需要跑一次 A/A 测试，既节省时间也节省资源。



> 如果有条件、有时间的话，推荐同时用统计公式和 Bootstrapping 两种方法分别计算方差。
>
> 如果两者的结果差距较大，就需要再去跑更多的 A/A 测试，所以从两方面验证得到的结果会更保险。



---

## 均值类、概率类指标统计属性

理解指标的统计属性，是我们掌握假设检验和 A/B 测试的前提。因为选取哪种检验方法，取决于指标的统计属性。

在实际业务中，我们常用的指标其实就是两类:

- **均值类的指标**，比如用户的平均使用时长、平均购买金额、平均购买频率，等等。
- **概率类的指标**，比如用户点击的概率(点击率)、转化的概率(转化率)、购买的概率 (购买率)，等等。



这些指标都是用来表征用户行为的。而**用户的行为是非常随机的**，这也就意味着这些指标是由一系列随机事件组成的变量，也就是统计学中的随机变量(Random Variable)。

在统计学中，表征随机变量的方法是通过概率分布**(Probability Distribution)**，来表征随机变量取不同值的 概率和范围。所以，A/B 测试指标的统计属性，其实就是要看这些指标到底服从什么概率分布。



> 💡 **在数量足够大时，均值类指标服从正态分布; 概率类指标本质上服从二项分布，但当数量足够大时，也服从正态分布。**



> 💡 中心极限定理(Central Limit Theorem) 这其实是均值类变量的特性:当样本量足够大时，均值类变量会趋近于正态分布。这背后的理论基础，就是中心极限定理。不管随机变量的概率分布是什么，只要取样时的样本量足够大，那么这些样本的平均值的分布就会趋近于正态分布。统计上约定俗成的是，样本量大于 30 就属于足够大了。在现在的大数据时代，我们的样本量一般都能轻松超过 30 这个阈值，所以均值类指标可以近似为正态分布。



> 💡 在二项分布中，有一个从实践中总结出的经验公式: $\min(np,n(1-p)) >= 5$。其中，n 为样 本大小，p 为概率的平均值。 这个公式的意思是说，np 或者 n(1-p) 中相对较小的一方大于等于 5，只有二项分布符合这个公式时，才可以近似于正态分布。这是中心极限定理在二项分布中的变体。



---



## 如何选取实验单位

> 理解误区：实验单位不就是用户吗?
>
> 除了测试系统的表现外，在绝大部分情况下，准确地说，实验单位都是用户的行为。因为**我们在产品、营销、业务上所做的调整，本质上都是为了观察用户的行为是否会有相应的变化**。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321190625176.png" alt="image-20230321190625176" style="zoom:50%;" />





### 用户层面

用户层面是指，把**单个的用户作为最小单位**，也就是以用户为单位来划分实验组和对照组。 



- **用户ID：** 用户注册、登录时的用户名、手机号、电子邮箱，等等 

  >  特点：稳定，不易改变

- **匿名ID：** 用户浏览网页时产生的cookies

  > - Cookies 是用户浏览网页时随机生成的，并不需要用户注册、登录。
  > - Cookies 一般 不包含个人信息，而且可以被抹除，因此准确度不如用户 ID 高。
  > - Cookies 仅限于该操作系统 内部，和用户浏览时使用的设备或者浏览器有很大关系。

- **设备ID：** 

  - 和设备绑定的，一旦出厂就不可改变。
  - 如果用户和家人、朋友共享上网设备的话，它就不能区分用户了。

- **IP地址：** 

  - 和实际的地理位置以及使用的网络都有关系。
  - 同一个用户，即使用同一个设备，在不同的地方上网，IP 地址也是不同的。
  - 在一些大的互联网提供商中，很多用户往往共享一个 IP 地址。所以，IP 地址的准确度是最差的， 一般只有在用户 ID、匿名 ID 和设备 ID 都得不到的情况下，才考虑使用 IP 地址。



### 访问层面 

把**用户的每次访问**作为一个最小单位。

> 我们怎么定义一次访问的开始和结束呢?
>
> - 访问的开始很好理解，就是进入到这个网站或者 App 的那一瞬间。
> - 难点就在于怎么定义 一次访问的结束。在一次访问中，我们可能会点开不同的页面，上下左右滑动一番，然后退出; 也有可能只是访问了一下没有啥操作，甚至都没有退出，就进入了其他的页面或者 App。



如果一个用户经常访问的话，就会有很多个不同的访问 ID。那在进行 A/B 测试的时候，如果以访问层面作为实验单位，就可能会出现**一个用户既在实验组又在对照组的问题**。

> 比如，我今天和昨天都访问了极客时间 App，相当于我有两个访问 ID，如果以访问 ID 作为实验单位的话，我就有可能同时出现在对照组和实验组当中。



### 页面层面

页面层面指的是把每一个**新的页面浏览**为最小单位。

关键词“新的”: 它指的是即使是相同的页面，如果它们被相同的人在**不同的时间**浏览，也会被算作不同的页面。

> 举个例子，我先浏览了极客时间的首页，然后点进一 个专栏，最后又回到了首页。那么如果以页面浏览 ID 作为实验单位的话，这两个首页的页面浏览 ID 就有可能一个被分配到实验组，一个被分配到对照组。



### 三种层面的对比

1. 从变化是否易被察觉考虑：

   - 访问层面和页面层面的单位，比较适合变化不易被用户察觉的 A/B 测试，比如测试算法的改进、不同广告的效果等等;

   - ⭐ **如果变化是容易被用户察觉的，那么建议你选择用户层面的单位**。不然可能会使得同一个用户，一下体验到好用的新功能，一下功能又没了，会让用户感到困惑、甚至沮丧，影响体验。

2. 实验单位的细粒度：

   - 从用户层面到访问层面再到页面层面，实验单位颗粒度越来越细，相应地可以从中获得更多的样本量。

   - 一个用户可以有多个访问，而一个访问又可以包含多个页面浏览。



> **在改动容易被察觉的情况下，以用户层面的实验单位的实验可能短时间没法获取足够样本量，在这种情况下，如果样本量不足，那就要和业务去沟通，明确样本量不足，需要更多的时间做测试，而不是选取颗粒度更小的单位。**
>
> 如果不能说服业务方增加测试时间的话，我们就要通过其他方法来弥补样本量不足会给实验造成的影响，比如：
>
> - 增加这次 A/B 测试使用的流量在总流量中的比例
> - 选用波动性(方差)更小的评价指标等方法



### 总结选取实验单位的原则



1. 保证用户体验的连贯性。

   > 同一个用户同时出现在实验组和对照组，就会体验到不同的功能、得到不同的体验。这种体验的不连贯性，就会给用户带来困惑和沮丧，很容易导致用户流失。

2. 实验单位应与评价指标的单位保持一致。

   >  A/B 测试的一个前提是**实验单位相互独立且分布相同的**，简称 IID。如果两个单位不一致，就会违反相互独立这一前提，破坏了 A/B 测试的理论基础，从而导致实验结果不准确。

3. 样本数量要尽可能多。

   > 在 A/B 测试中，样本数量越多，实验结果就越准确。但增加样本量的方法有很多，我们绝 对不能因为要获得更多的样本量，就选择颗粒度更细的实验单位，而不考虑前面两个原 则。



---

## 最小样本量问题



样本量越大，样本所具有的代表性才越强。**但在实际业务中，样本量其实是越少越好。**

因为当样本数量很少的时候，实验容易被新的样本点带偏，造成了**实验结果不稳定**，难以得出确信的结论。相反的，样本数量变多，实验说服性也更强。



A/B 需要做多长时间的一个公式: **A/B 测试所需的时间 = 总样本量 / 每天可以得到的样本量。**

在现实操作中，样本量应该越少越好，这是因为：

1. 流量有限

   - 公司流量有限，不合理分配流量，产品迭代速度会大大降低

     > 从公式就能看出来，样本量越小，意味着实验所进行的时间越短。在实际业务场景 中，时间往往是最宝贵的资源，毕竟，快速迭代贵在一个“快”字。

2. 试错成本大

   > 如果使用50%的流量进行实验，一周后结果表明实验组的总收入下降了20%。算下来，实验在一周内给整个公司带来了10%的损失。试错成本太高。
   >
   > **实验范围越小，样本量越小，试错成本就会越低**

   

实践和理论上对样本量的需求，其实是一对矛盾。所以，我们就要在统计理论和实 际业务场景这两者中间做一个平衡：**在 A/B 测试中，既要保证样本量足够大，又要把实验控制在尽可能短的时间内。**



### 样本量计算公式

需要计算满足实验要求的最小样本量，最小样本量是**根据统计功效进行计算的**

主要分两类：绝对值类（例如：UV）和比率类（例如：点击率）

- 显著水平 $\alpha$: 一般越小越好，一般不大于5%
- 统计功效 $1-\beta$: 越大越好，一般不低于80%，即 $\beta$ 小于20%

$$
\mathrm{n}=\frac{\left(Z_{1-\frac{\alpha}{2}}+Z_{1-\beta}\right)^{2}}{\left(\frac{\delta}{\sigma_{\text {pooled }}}\right)^{2}}=\frac{\left(Z_{1-\frac{\alpha}{2}}+Z_{\text {power }}\right)^{2}}{\left(\frac{\delta}{\sigma_{\text {pooled }}}\right)^{2}}
$$



> 其中:
>
> - $Z_{1-\frac{\alpha}{2}}$  为  $\left(1-\frac{\alpha}{2}\right)$  对应的  Z  Score。
> - $Z_{\text {Power }}$  为 Power 对应的 Z Score。
> - $\delta$  为实验组和对照组评价指标的差值。
>   - 如果两个版本的均值差别巨大，也不太需要多少样本，就能达到统计显著
>   - 两组数值的差异，如点击率1%到1.5%，那么Δ就是0.5% （这个差异需要根据实施变化后所需成本和收益是否达到预期来估算）
> - $\sigma_{\text {pooled }}^{2}$  为实验组和对照组的综合方差 (Pooled Variance)。
>   - 组间的标准差越小，代表两组差异的趋势越稳定。越容易观测到显著的统计结果
>
> 
>
> 在公式中，样本量主要由统计显著性$ \alpha$, 统计功效 Power, 评价指标差值 $\delta$ 和 综合方差 $\sigma_{pooled}$决定。因此样本量大小的调整依靠这四个因素。



### 四个因素和样本量 n 的关系

1. 显著水平(Significance Level) $α$

   > 显著水平和样本量成反比:显著水平越小，样本量就越大。这个也不难理解。
   >
   > 因为显著水平又被称为第一类错误率(Type I Error) **α**，想要第一类错误率越小，结果越精确，就需 要更大的样本量。

2. Power$ (1 – β)$

   > Power 越大，样本量就越大。Power 越大，就说明第二类错误 率(Type II Error)β越小。
   >
   > 和第一类错误类似，想要第二类错误率越小，结果越精确，就需要更大的样本量。

3. 实验组和对照组的综合方差 $\sigma_{\text{pooled}}^2$

   - 方差和样本量成正比:方差越大，样本量就越大。

   - 方差是用来表征评价指标的波动性的，方差越大，说明评价指标的波动范围越 大，也越不稳定，那就更需要更多的样本来进行实验，从而得到准确的结果。

4. 实验组和对照组评价指标的差值 $δ$

   > 差值和样本量成反比：差值越小，样本量就越大。因为实验组和对照组评价指标的差值越小，越不容易被 A/B 测试检测到，所以我们需要提高 Power，也就是说需要更多的样本量来保证准确度。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321193620306.png" alt="image-20230321193620306" style="zoom:50%;" />



### 实践中如何计算样本量

绝大部分的 A/B 测试都会遵循统计中的惯例：**把显著水平设置为默认的 5%， 把 Power 设置为默认的 80%。** 

这样的话我们就确定了公式中的 Z 分数，而且四个因素也确定了两个(α、Power)。那么，样本量大小就主要取决于剩下的两个因素:实验组和对照组的综合方差 $\sigma_{\text{pooled}}^2$ ，以及两组评价指标的差值 $δ$。

因此样本量计算公式可以简化为：$n\approx \frac{8\sigma_{\text{pooled}}^2}{\delta^2}$

以上公式其实是在两组评价指标的综合方差为 $\sigma^2$ ，两组评价指标的差值为 $\delta$ 情况下，要使 A/B 测试结果**达到统计显著性的最小样本量。**



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321193250662.png" alt="image-20230321193250662" style="zoom:50%;" />



重点强调“最小”二字。理论上样本量越大越好，上不封顶，但实践中样本量 越小越好，那我们就需要在这两者间找一个平衡。所以由公式计算得到的样本量，其实是平衡二者后的最优样本量。



### 实验组和对照组的样本量分配问题

实验组和对照组的样本量应保持相等：

- 如果 A/B 测试的实验组和对照组样本量相等，即为 50% & 50% 均分，那么我们的总样本量 (实验组样本量和对照组样本量之和)为:  $n_{\text{total}}\approx \frac{8\sigma_{\text{pooled}}^2}{\delta^2}\times 2 \approx \frac{16\sigma_{\text{pooled}}^2}{\delta^2}$
- 计算的达到统计显著性的最小样本量，是以每组为单位的，并不是以总体为单位。也就是说，**在非均分的情况下，只有相对较小组的样本量达到最小样本量，实验结果才有可能显著，并不是说实验组越大越好，因为瓶颈是在样本量较小的对照组上。**
- **只有两组均分，才能使两组的样本量均达到最大，并且使总样本量发挥最大使用效率，从而保证A/B 测试更快更准确地进行。**



### 估算实验组和对照组评价指标的差值$δ$

不会事先知道实验结束后的结果,但可以通过一些方法估算：

1. 从收益和成本的角度进行估算。考虑变化带来的总收益能否抵消掉成本，达到净收益为正呢?

   > 我们进行相应的改变肯定是希望获得净收益，所以一般我们会算出当收支平衡时差值为 $\delta_{\text{收支平衡}}$, 希望差值 $\delta \ge  \delta_{\text{收支平衡}}$.

   

2. 如果收益和成本不好估算的话，我们可以从历史数据中寻找蛛丝马迹: 算出这些评价指标的平均值和波动范围， 从而估算一个大概的差值。

   > 通过历史数据算出点击率的平均值为 5%，波动范围是 [3.5%, 6.5%]，那么我们对实验组评价指标的期望值就是至少要大于这个波动范围，比如 7%，那么这时$δ$就等于 2%(7%–5%)。



### 计算实验组和对照组的综合方差$\sigma_{\text{pooled}}^2$

选取历史数据，根据不同的评价指标的类型，来选择相应的统计方法进行计算。



评价指标的类型主要分为概率类和均值类这两种。

- 概率类指标在统计上通常是二项分布，综合方差为:
  - $\sigma_{\text {pooled }}^{2}=p_{\text {test }}\left(1-p_{\text {test }}\right)+p_{\text {control }}\left(1-p_{\text {control }}\right)$
- 均值类指标通常是正态分布，在样本量大的情况下，根据中心极限定理
  - $\sigma_{\text {pooled }}^{2}=\frac{2 * \sum_{i}^{n}\left(x_{i}-\bar{x}\right)^{2}}{n-1}$



### 绝对值类

均值类假设检验形式通常为:
$$
\begin{array}{l} H_{0}: \mu_{A}-\mu_{B}=0 \\ H_{1}: \mu_{A}-\mu_{B} \neq 0 \end{array}
$$


故对应的样本量计算公式为:
$$
n_{A}=\kappa n_{B} \text { and } n_{B}=\left(1+\frac{1}{\kappa}\right)\left(\sigma \frac{z_{1-\alpha / 2}+z_{1-\beta}}{\mu_{A}-\mu_{B}}\right)^{2}
$$


其中, 两组样本量之比为 $\kappa=\frac{n_{A}}{n_{B}}$

统计功效的计算公式为:
$$
1-\beta=\Phi\left(z-z_{1-\alpha / 2}\right)+\Phi\left(-z-z_{1-\alpha / 2}\right) \quad, \quad z=\frac{\mu_{A}-\mu_{B}}{\sigma \sqrt{\frac{1}{n_{A}}+\frac{1}{n_{B}}}}
$$

### 比例类

比例类假设检验形式通常为:
$$
\begin{array}{l} H_{0}: p_{A}-p_{B}=0 \\ H_{1}: p_{A}-p_{B} \neq 0 \end{array}
$$


故对应的样本量计算公式为:
$$
n_{A}=\kappa n_{B} \text { and } n_{B}=\left(\frac{p_{A}\left(1-p_{A}\right)}{\kappa}+p_{B}\left(1-p_{B}\right)\right)\left(\frac{z_{1-\alpha / 2}+z_{1-\beta}}{p_{A}-p_{B}}\right)^{2}
$$


统计功效的计算公式为:
$$
\begin{array}{c} 1-\beta=\Phi\left(z-z_{1-\alpha / 2}\right)+\Phi\left(-z-z_{1-\alpha / 2}\right) \\ z=\frac{p_{A}-p_{B}}{\sqrt{\frac{p_{A}\left(1-p_{A}\right)}{n_{A}}+\frac{p_{B}\left(1-p_{B}\right)}{n_{B}}}} \end{array}
$$


### 实验有效天数

实验的有效天数的确定需要考虑两个因素：

- 试验进行多少天能达到流量的最小样本量

- 同时还要考虑到用户的行为周期和适应期

  > **用户的行为周期**
  >
  > 部分行业用行为存在周期性，例如电商用户购买行为，周末与工作日有显著差异。故实验有效天数应覆盖一个完整的用户行为周期。
  >
  > **用户适应期**
  >
  > 如果进行的样式改版一类的实验，新版本上线用户会因为新奇效应而存在一定得适应期。故应考虑适应期在实验有效天数内，然后再分析实验结果。适应期的长短通常以足量用户流量参与试验后的2到3天为宜。



### FAQ

**1. 最小组的样本量没有达到最小样本量会如何？** 

> 最小组的样本量未达到最小样本量的话，得出的结果出现**假阳性**的概率就会增大，那么结果的可信度就会降低，所以很不推荐。



**2. 实验组、对照组样本量不均分可以吗**

> 如果样本量不是均分的话，其实理论上来说方差是要算unpooled variance的，不过其实一般都可以用pooled variance近似的，最小组的样本量也达到最小样本量结果是可信的。



---



## 流量分配

### 随机分流

所谓**“同质性”**就是保证对可能影响实验指标的因素，实验组和对照组应该是一致的。 

为了保证不同的组的样本的“同质性”，最简单的办法就是随机，将总样本完全随机分成不同组，基本上能保证不同组样本的同质。这也是业内目前最常见的A/B实验分流算法——**随机分流**。

随机分流算法是业内最常见的A/B实验分流算法，能够满足大部分A/B实验场景，比如广告投放、UI样式、营销派券等场景对A/B实验的诉求。随机分流算法的设计也较为简单，通过**将分流ID（用户ID/司机ID等）随机分到不同组的任意一组**即可。

> 但需要特别注意的是，A/B实验一般都需要满足：进组的用户不再出组，即：同一个分流id，在不调整分组流量占比的情况下，无论多少次进入A/B实验，都应该在同一个分组。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321142813890.png" alt="image-20230321142813890" style="zoom:50%;" />



### 分流与分桶原理

需要保证：

- 同一实验中不同分桶之间是随机的；
- 不同的场景、实验，分桶会被重新打散；
- 实验设计时，需要考虑验证哪个因子，则可以按照那个因子来进行分桶；



分桶和分流之间的关系：

- 分流是指，从总体中随机抽样百分之几来做实验；

- 分桶是指，在实验的流量里面根据某个需要验证的因子随机分桶；

  > 例如，将所有用户按照性别进行分类，然后将男性用户随机分配到A组或B组，女性用户也随机分配到A组或B组。



### 流量分层

通常由于流量有限，以及同时进行的AB实验量比较大，为了保证流量高效使用需要利用**分层**和**分流**的流量分配机制。

根据不同的实验共享流量的情况下，不同的实验之间是否会产生干扰，将实验类型分为**正交实验**和**互斥实验**

为了更充分、更高效的使用流量，实际往往是多组试验同时存在，既有正交，又有互斥。

流量分层的意义：

- 多个AB实验同时做，如何分流才能保证实验互不受影响？
  - **首先，考虑到实验迭代效率，实验之间的流量不可能完全独立。** 
    - 很多业务团队可能都需要大盘的流量做实验，而实验流量总体是有限的，另一方面实验时间、效率也是有要求的
  - 如何在**保证实验迭代效率的同时，又保证实验之间的结果不受影响**呢？ 
    - 流量分层来解决：各分层之间的流量是正交的，可以保证不同流量层的实验不会互相影响。在这个原理下，没有相关性的实验，可以开在不同流量层，极大的拓展了可同时实验的数量上限，不过有相关性的实验必须开在相同层。



#### **正交实验**

> 正交是指用户进入所有的实验之间没有必然关系。比如进入X层的用户再进入Y层也是均匀分布的，而不是集中在某一块区间内。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321143836278.png" alt="image-20230321143836278" style="zoom:50%;" />

正交实验的意义：各分层之间的流量是正交的，可以保证不同流量层的实验不会互相影响。将一个实验A的实验组和对照组的流量随机均匀分给另一个实验B的实验组和对照组，由于分配是均匀的，所以实验A对实验B的影响被均匀打散，从而避免实验A对实验B的结果产生影响。



#### **互斥实验**

> 指两个实验流量独立，用户只能进入其中一个实验。比如进入X实验的用户就不能进入Y实验。

<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321144008107.png" alt="image-20230321144008107" style="zoom:50%;" />



### 分流算法

我们如何实现正交和互斥实验的随机均匀分流呢？通常我们用的是**Hash算法**。



### Summary ⭐️

在 AB 测试中，流量分割策略是指**如何将流量平均分配到控制组（control group）和实验组（experiment group）中**。这是非常重要的，因为如果流量分配不均匀，就可能对结果造成影响。

下面是一些设计流量分割策略的建议：

1. **尽量平均地分配流量**：尽量将流量平均分配到控制组和实验组中，这样可以减少因流量分配不均匀对结果的影响。
2. 考虑流量来源：如果流量来源不同，则可能会对结果产生影响。因此，应尽量使流量来源相似，例如，如果有**多个渠道**，则应尽量使每个渠道的流量比例相似。
3. 考虑流量特征：如果流量中存在某些特征，例**如地区、设备类型、浏览器等**，则应尽量使控制组和实验组的**流量特征**相似。
4. 使用随机分配：可以使用随机分配的方法将流量分配到控制组和实验组中，这样可以最大程度地**减少因人为因素对结果的影响**。



---



## 实验组、对照组的独立性

### 为什么要强调两组的对立性

AB测试的前提：实验组和对照组的实验单位是要相互独立的，意思是说测试中各组实验单位的行为仅受本组体验的影响，不能受其他组的影响。这个前提又叫做 Stable Unit Treatment Value Assumption (SUTVA)

在实验过程中，很可能实践中碰到一些业务场景，破坏了两组的独立性，导致实验准确性被破坏。

因为AB测试的本质是因果推断，只有在实验组和对照组互相独立、互不干扰前提下，才能将测试结果的显著不同，归因为实验组相对对照组的变化，否则无法准确建立因果关系。



### 破坏独立性的表现形式

#### 社交网络、通讯

业务：用户间的交流和信息交换，包括微信、微博、领英、语音/视频通讯、电子邮件等

这类业务会存在网络效应，即网络中相邻的节点可能会互相影响，如节点A在实验组，相邻点B在对照组，两者就不是独立的

> 例如：实验组A感受到产品的改进后再朋友圈分享，结果对照组B看到分享后，使用产品的频率更多



#### 共享经济

业务：一般为双边市场（Two-Sided Market），公司只提供交易平台，供给方和需求方都是用户，典型淘宝、滴滴、Uber、共享单车、共享租赁、Airbnb等

在这类业务中，由于**供需关系是动态平衡的，一方的变化必然会引起另一方的变化**，从而造成实验中两组相互影响。

> 例子1
>
> 我们在用 A/B 测试验证不同的优化是否有效时，往往只能一次验证一个优化。如果我们用 A/B 测试检验一个需求侧的优化，就要在需求侧分成实验组和对照组，这样实验组由于受到了优化，就会导致需求增加。那么在供给一定的情况下，更多的供给流向了实验组，就会造成对照组的供给减少，对照组的用户体验会变得更差，从而进一步打击对照组的需求。



> 例子2 
>
> 某共享打车服务优化了用户在 App 中的打车流程，现在我们要通过 A/B 测试来验证这个优化是否有效果。 这里，实验组依然是使用优化流程，对照组则使用旧流程。实验组的用户因为流程的优化，打车更加方便，吸引了更多的司机。而由于司机的数量是稳定的，这就会导致可供对照组选择的司机减少，对照组的用户更难打到车，用户体验变差，那么通过 A/B 测试得出的流程优化的效果相比较对照组就会被高估。



#### 共享资源

业务：有固定的资源或者预算，最常见的就是广告营销了。

> 例子
>
> 在营销预算固定的情况下，我们用 A/B 测试来验证不同广告的效果。如果发现我们在实验组改进后的广告效果更好，点击率更高，那么这就会造成对照组的广告预算减少，从而影响到对照组的广告效果。因为线上的广告大部分是按点击次数付费的，所以这时候实验组 广告花的钱就越多，在营销预算固定的情况下就会抢占对照组的预算。以此来看，通过 A/B 测试得出的实验组的广告效果就会被高估。



> **网络效应**
>
> 产品可以划分为「单边产品」和「多边产品」。
>
> **单边产品**，例如：搜索引擎，用户使用之间是不会相互干扰的；
>
> **多边产品**：例如：聊天软件，用户之间存在互通，一方影响会传导到另外一方。
>
> 而这种「多边产品」用户之间相互影响的情况，我们称之为网络效应。
>
> 例如，社交性产品由于存在交互性，这种AB组用户直接或间接干扰的情况，则是网络效应对实验的影响。
>
> 
>
> **那么针对存在网络效应的实验，要如何开展实验呢？关注以下两步：**
>
> **步骤一**：将用户按照地理位置、社会群体等方式进行用户群体切割，AB桶分别选择相对独立的两个群体。
>
> **步骤二**：由于群体之间是存在差异性的，因此需要尽可能选择相似的用户群体，结合PSM等方式进行评估。例如：按照地理位置划分，假设北京和上海在重点关注的特征上比较相近，则可以作为实验的AB组。



### 如何避免独立性被破坏

总的原则就是**通过不同形式的分离来排除两组之间的干扰。分离方法基本有4种：**



#### 从地理上分离

1. 主要适用于受到地理位置影响的线下服务，比如共享出行和共享租赁，这种本地化的服务一般不同的地域之间不会有干扰，这时候就可以按照不同的市场来分类。
2. 常用从城市维度分类，如北京用户为实验组，上海用户为对照组，排除组间干扰
3. 注意点：不同市场要尽可能相似，否则不具有可比较性，相似性指代当地发展情况、当地经济情况、人口分布情况等



#### 从资源上分离

1. 主要适用于由于共享资源造成的两组之间的干扰。
2. 操作：A/B 测试中每组 的资源分配比例要和每组样本量的比例一致。比如在做广告营销中，如果通过 A/B 测试比 较不同组的广告的效果，那么每组分配的广告预算的比例要和每组的样本量比例相等，比 如两组样本量均分时，广告预算也要均分，这样两组之间的广告预算才能互不干扰。



#### 从时间上分离

1. 主要适用于不易被用户察觉的变化上，比如算法的改进

2. 原理： 实验组和对照组都是同一组用户，在一段时间内实施变化，给他们实验组的体验，然后在另 一段时间内不实施变化，给他们对照组的体验。

3. 时间单位：分钟、小时、天

   如果由于实验组的体验不好造成了用户的流失，如果真的出现这种情况的话，可以适当缩短时间单位(比如把10天变为1天，1小时，1分钟等等)，然后加大变化的频率，这样的话能够减少影响。

4. 注意点：注意周中、周末的周期性波动



#### 通过聚类方法分离 （难）

1. 主要适应于社交网络类业务。
2. 社交网络中用户之间的连接其实也不是均匀的，有远近亲疏，那就可以通过模型的方法，根据不同用户之间交流的程度来分离出不同集群cluster，每个 cluster 都会有不同的联系很紧密的用户，我们可以把每个 cluster 作为实验单位随机分组，这样就能从一定程度上减少不同组之间的干扰。



---



## 结果分析

### 结论分析过程

做结论的过程：

1. 一般来说，ab 测试有四类指标（不变指标）

2. 选择参数：选择显著性水平，统计力量和实际意义水平

3. 计算所需的样本量 （例如主题测试，人口测试）

4. 为对照/治疗组取样本并进行测试 （持续时间，曝光程度和学习效果）

5. 分析结果并得出结论

   - 完整性分析检查：检查你的不变指标是否已更改

   - 分析结果：第一轮检查是否真的没有显著差异，第二轮利用不同方法进行交叉检查

6. 得出结论

   - 你明白这个改变吗？你想推出改变吗？我该如何决定是否启动更改？

   - 问自己，我了解实际对我们的用户体验所做的更改吗？我是否具有统计意义和实际意义的结果，以证明变更的正确性？

   - 最后但并非最不重要的是，它是否值得冒险？



### 完整性检查

进行实验结果评估前， 应该先进行完整性检查（Sanity Checks），确保已经恰当地完成了实验。

实验中有太多环节可能导致实验结果是无效的。例如实验分配失误，导致实验组和对照组无法进行对比；又或者数据收集过程出错了。在评估前，应该对实验检查。

如果完整性检查都失败，则可能背后的实验设计、基础设施或数据处理都是有问题的。

- 检查一些不变量是不是在实验中真的没有发生变化。例如实验单元数量是否在实验前后大致相同。
- 如果检查失败了，则不再对实验结果分析，而是直接分析为什么完整性检查失败了。
  - 可能是技术上的错误，与工程师一起检查，可能是实验架构出错
  - ‼️ 回顾性分析，尝试从数据采集方面重新进行实验分组
  - 利用在实验前后、实验中对比, 可能是实验设置出错



### 不变量指标→完整性、合理性检查

许多漏洞可能会导致实验结果失效，为抓住漏洞，可以关注一些护栏指标（guardrail metric）或不变量（invariant）。这些指标不应该在对照组和实验组之间存在差异。



一般来说，ab 测试有四类指标（不变指标）：

1. 总数和数量
2. 分布（平均数，中位数，百分位数）
3. 概率（点击率和点击概率）
4. 比率：任意两个数字互相粘性



### 单个指标

1. 判断改动是否具有统计显著性
2. 非参数符号检验，并与参数检验的结果对比



### 多个指标

如何根据多个指标，给出指导意见？

- 多个指标是难以控制的



---



## 测试结果不显著，如何解决

### 为什么会出现实验结果不显著

- A/B 测试中的变化确实没有效果，所以两组的指标在事实上是相同的。
- A/B 测试中的变化有效果，所以两组的指标在事实上是不同的。但是由于变化的程度很 小，**测试的灵敏度，也就是 Power 不足**，所以并没有检测到两组指标的不同。



> 如果是第二种原因，那我们可以从 A/B 测试的角度进行一些优化和调整。具体来说就是， 通过提高 Power 来提高 A/B 测试检测到实验结果不同的概率。Power 越大，越能够准确地检测出实验组与对照组的不同。所以当我们提高了 Power 之 后，如果仍然发现测试结果不显著，这样才能得出“两组指标事实上是相同的”的结论。



### 如何提高统计功效

$$
\mathrm{n}=\frac{\left(Z_{1-\frac{\alpha}{2}}+Z_{1-\beta}\right)^{2}}{\left(\frac{\delta}{\sigma_{\text {pooled }}}\right)^{2}}=\frac{\left(Z_{1-\frac{\alpha}{2}}+Z_{\text {power }}\right)^{2}}{\left(\frac{\delta}{\sigma_{\text {pooled }}}\right)^{2}}
$$



从计算样本量的公式来看, 影响Power的因素有：

1. 样本量：样本量和 Power 成正比。即通过增大样本量就可以提高 Power。
2. 方差：方差和 Power 成反比。即通过减小方差就可以提高 Power。



具体来说，实践中：

- 在有条件获得更大样本量的情况下，可以选择增大样本量的方法来提高 Power，相对简单易操作。
- 如果受流量或时间限制，没有条件获得更多的样本量，此时可以通过减小方差来提高 Power。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321200417312.png" alt="image-20230321200417312" style="zoom:50%;" />



### 如何通过增加样本量来提高 Power

1. **延长测试时间**

   每天产生的可以测试的流量是固定的，那么测试时间越长，样本量也就越大。所以在条件允许的情况下，可以延长测试的时间。

   

2. **增加测试使用流量在总流量中的占比**

    假设某个产品每天有 1 万流量，如果我要做 A/B 测试，并不会用 100% 的流量，一般会用总流量的一部分，比如 10%，也就是测试使用流量在总流量中的占比。

   1. 考虑试错成本：使用的流量越少，试错成本越低，也就越保险。

   2. 考虑产品的媒体效应：在大数据时代，对于互联网巨头来说，由于本身就拥有巨大的流量，那么产品 本身做出的任何比较明显的改变，都有可能成为新闻。

      

3. **多个测试使用同一对照组**

   1. 增加每组的流量利用率
   2. 在同一个基础上想同时验证多个变化，也就是跑多个 A/B 测试有相同的对照组的时 候，我们可以把对照组合并，减少分组数量，这样每组的样本量也会增加。这种测试又叫 做 A/B/n 测试。



### 如何通过减小方差来提高Power

1. **减小指标的方差**

   - 保持原指标不变，通过剔除离群值（outlier)来减小方差
     - 通过设定封顶阈值(Capping Threshold)的方法把离群值剔除掉。

   - 选用方差较小的指标

2. 倾向得分匹配(Propensity score matching): 因果推断的一种方法，目的是解决实验组和对照组分布不均匀的问题。

   - **两组的各个特征越相似，就说明两组的方差越小。**

   - 倾向评分越接近，说明两个数据点越相似。

   - PSM具体算法：

     - 把我们要匹配的两组中每个数据点的各个特征(比如用户的性别，年龄，地理位 置，使用产品 / 服务的特征等)放进一个逻辑回归(Logistics Regression)中。

     - 计算得到的logistics得分即为1. 每个数据点的倾向评分

     - 通过最近邻等方法对相近的倾向得分对应的样本点匹配

     - 最后我们只需要比较匹配后的两组相似的部分即可。

   - **PSM 能够有效地减少两组的方差。通过比较倾向评分匹配后的两组的相似部分，我们可以来查看结果是否显著。**

3. 在触发阶段计算指标： 在 A/B 测试中我们把实验单位进行随机分组的这个过程叫做分配(Assignment)。我们要测试的变化是需要满足 一定条件才能触发的。

   - 变化不需要条件触发。所有用户在被分配到实验组后，就都可以体验到 A/B 测试中的变化。

   - 变化需要条件触发。在被分配到实验组的所有用户中，只有满足一定条件的用户才会触 发 A/B 测试中的变化。

<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321200616206.png" alt="image-20230321200616206" style="zoom:50%;" />



---

## 灰度测试

### 灰度发布流程/灰度上线是怎么操作的

1. 定义目标
2. 选定策略：包括用户规模、发布频率、功能覆盖度、回滚策略、运营策略、新旧系统部署策略等
3. 筛选用户：包括用户特征、用户数量、用户常用功能、用户范围等
4. 部署系统：部署新系统、部署用户行为分析系统（web analytics）、设定分流规则、运营数据分析、分流规则微调
5. 发布总结：用户行为分析报告、用户问卷调查、社会化媒体意见收集、形成产品功能改进列表
6. 产品完善
7. 新一轮的灰度发布，循环



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321202817916.png" alt="image-20230321202817916" style="zoom:50%;" />



### 灰度发布时，目标用户的选取策略

即应该如何选取哪些用户优先体验新版本，是强制升级还是让用户自主选择等。可以考虑的因素有：

1. 用户的地理位置
2. 用户终端特性（如分辨率、性能等）
3. 用户自身特性（性别、年龄、忠诚度等）



强制升级：

1. 一些细微的修改，例如文案、少量控件的位置调整等



可自主选择的升级：

1. 例如微博改版，应该让用户自主选择，并要提供用户一个可以自主回滚到旧版本的入口
2. 对于客户端的应用，例如浏览器、vim编辑器这样的软件，可以让用户自主选择采用stable稳定版、beta测试版等版本，在用户有明确预期的情况下自行承担试用风险。



### 比较AB测试和灰度测试的不同

**不同点**

- AB测试

  - 通常由产品经理和运营来主导
  - 将两种功能或两个版本，交给无差别的用户群体选择
  - AB测试上线的功能或版本一定是可用的

  

- 灰度测试

  - 一般由研发，测试或运维来主导。
  - 将系统的新版本，或者说新功能，以部分上线的方法来上线，验证新版本是否足够可靠。
  - 灰度版本未必是可用的，或者说没有严重bug的
  - 主要用于上线前的测试，收集用户反馈。对某一产品、功能、版本的发布是逐步扩大适用群体范围，也称为**灰度放量**

**相同点：**

- 都是为了保证产品、整体系统的稳定性，降低产品升级影响的用户范围



### 灰度测试优点

- 通过正式上线前的测试，提前获得用户的使用反馈，减小风险影响的范围
  - 尽早获得用户的意见反馈，完善产品功能，提高产品质量
  - 让用户参与到产品测试中，加强了产品与用户之间的互动
  - 降低产品升级所影响到的用户范围
- 可按地域、人群、时段等自定义标签对产品功能或者网页页面进行内容的精准投放



### 常见问题

![image-20230321203002676](/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321203002676.png)



---

## 实验注意点

### 实验对象选择→随机化单元是什么

选择实验对象是谁： subject，不同的分流方式

对事件分组→分组单元，unit of diversion

1. 用户ID user id:  用户获得的体验是连贯的

   - 稳定且不易变动的：stable and unchanging

   - 个体识别性高： personally identifiable

2. 网页cookie (anonymous id)

   - cookie: 作为设备上某文件的匿名随机标识

   - 当更换浏览器或者设备时，cookie也会改变

   - 用户可以清除cookies

3. 事件 event

   - 非一致性体验: non consistent experience

   - 只对用户不可见的改动可用

4. Device id:

   - 只对移动端有

   - 试图指定特定的设别

   - 用户不可改变

   - 个体可识别性

5. IP address:

   - 当地理位置改变时，也跟着改变



要点：

1. **保证consistency of diversion.**
2. 保证ethical问题，道德问题→知情同意 （ethical review)
3. variability 差异性问题： 分析单元(unit of analysis)，即所有指标的分母



> 对于用户可见的改动，一定会使用cookie或者用户ID。



### 总体人群→目标群体是什么

选择总体人群: population

要点：

1. 筛选实验人群，也可能导致差异性的出现 → Global data 和 local data



如何限制实验人群：

1. 限制语言
2. 限制浏览器类型



队列(cohort): 同时进入实验的对象们。仅关注大约在相同时间加入实验组和对照组的用户，并从那个时间点开始实验。

什么时候用总体，什么时候用队列？

- 队列的用户数一般会更少一点，会需要更多的数据
- 一般仅会在观察用户稳定性时使用队列



### 样本量大小→实验需要多大样本量

- 样本量大小: Size

> 💡 为能以80%的统计功效检测出至少1%的指标变化，通过功效分析来确定样本量。



如何确定实验组和对照组的规模？

- **这是一个迭代的过程**
- variability会影响sizing
- 考虑实验的安全性：安全性会影响放量策略
- 实验是否需要和其他实验共享流量？如何平衡流量需求？



### 实验持续时间→实验要运行多久

- 实验持续的时间： Duration

- 什么时候开始实验

- 在实验期间要发送多少的流量

  - 例如实验一共需要100w的流量，假设一天网站浏览是10w,则可能需要运行10天，假设一天10w的流量还需要分给其他实验使用，例如50%，那还剩下50%的流量，则可能实验要进行20天。

  - 控制流量的比例很重要

    - 考虑安全性，流量太大，可能会让很多不喜欢改动的用户流失
    - 考虑周内效应：还可能有其他因素影响实验，流量太大是问题，可能遇到一些特殊节日使得流量出现异常，如节假日等。周末和周中访问的用户群体可能不一样。可能每天的流量也不同
    - 考虑季节性
    - 考虑用户
    - 考虑初始和新奇效应

    

> **AB实验周期如何选择？需要考虑哪些因素？过长或者过短会有什么影响** 
>
> 实验周期选择需要考虑三方面因素：
>
> 1. 考虑最小样本量。实验周期内累计样本量，需要大于最小样本量要求。
> 2. 考虑周末效应。一般产品周中和周末用户行为表现会存在差异，因此实 验至少需要运行完整一周。
> 3. 考虑新奇效应。重点针对老用户，改版会对用户产生非持久性的行为驱 动，这段时期的数据是缺乏置信度的，因此需要适当拉长实验周期。
>
> 在考虑以上因素的前提下，制定实验周期，时间过长会导致实验送代的效率 而时间过短会导致实验的不置信。

---

## 如何评估测试结果的准确性

分析A/B实验的定义，要实现科学权威的评估，最重要的两点在于：

- 第一，**确保在实验前分出无差别的实验组和对照组**，避免因流量分配不平衡导致的AB群组差异过大，最终造成对于实验结果的误判；
- 第二，确保对实验结果作出准确的判断，能够准确的判断新策略相对于旧策略的优势是**不是由自然波动引起的**，它的这一优势能否在大规模的推广中反映出来。



### 什么时候可以查看测试结果

什么时候可以查看测试结果，停止 A/B 测试呢? 这是保证测试结果可信赖要解决的第一个问题。

AB测试所需的时间：

1. 考虑测试达到显著性结果所需的最小样本量
2. 考虑指标周期性变化的因素



#### 测试结果在预计时间之前达到了统计显著，这个实验是不是提前成功了

答案当然是否定的。 一方面，因为样本量是不断变化的，所以每次观测到的测试其实都可以算作新的实验。根据统计上的惯例，A/B 测试一般有 5% 的第一类错误率$α$，也就是说每重复测试 100 次， 平均就会得到 5 次错误的统计显著性的结果。这就意味着如果我们观测的次数变多的话，那么观测到错误的统计显著结果的概率就会大大提升，这是**多重检验问题**(Multiple Testing Issue)的一种体现。

另一方面，提前观测到统计显著的结果，这就意味着样本量并没有达到事先估算的最小样本量，那么这个所谓的“统计显著的结果”就极有可能是错误的**假阳性**(False Positive)。“假阳性”是指，两组事实上是相同的，而测试结果错误地认为两组显著不 同。



#### 如果测试已经跑到了第 10 天，样本量也达到了之前计算的量，那是不是就可以开始分析 A/B 测试的结果了

答案依旧是不行。

为了确保实验在具体实施过程中按照我们预先设计的进行，保 证中途不出现 Bug，那么在正式分析实验结果前，我们还要进行测试的**合理性检验 (Sanity Check)**，从而保证实验结果的准确性。



### 合理性检验

1. 检验实验、对照组样本量的比例： 预设的是，实验组和对照组的样本量各占总样本量的 50%

   - 各组样本量占总样本量的比例也是概率，也是符合二项分布的：

     - 二项分布的标准误差： $\sqrt{\frac{p(1-p)}{n}}$。

     - 以 0.5(50%)为中心构建 95% 的置信区间。

     - 确认实际的两组样本量的比例是否在置信区间内。

   - 如果总的比例在置信区间内的话，就说明即使总的比例不完全等于 50%/50%，也是非常接近，属于正常波动，两组样本量大小就符合预期。否则，就说明实验有问题。

   - 实验 / 对照组样本量的比例和实验设计不相同时会出现样本比例不匹配问题(Sample Ratio Mismatch)

2. 检验实验、对照组中的特征分布 （检验两组中的特征分布是否相似，是否符合实验设计要求的比例分布。在不同细分领域中的分布也要均匀）

   - A/B 测试中实验组和对照组的数据要相似才具有可比性。这里的相似，我们可以通过比较 两组的特征分布来判断。

   - 常用的特征包括用户的年龄、性别、地点等基本信息，或者可能影响评价指标的特征。

   - 实验 / 对照组的特征分布不相似则会导致辛普森悖论问题(Simpson Paradox)

一旦从合理性检验中发现了问题，就不要急着分析实验结果了，实验结果大概率是不准确 的。我们要做的就是找到出现问题的原因，解决问题，并重新实施改进后的 A/B 测试。



### 如何找出实验中的问题

1. 和工程师一起从实施的流程方面进行检查，看看是不是具体实施层面上两组有偏差或者 bug。

2. 从不同的维度来分析现有的数据，看看是不是某一个特定维度存在偏差。

   > 常用的维度有 时间(天)、操作系统、设备类型等。



### 如何分析测试的结果

分析 A/B 测试的结果，主要就是对比实验组和对照的评价指标是否有显著不同。

那怎么理解“显著”呢?其实，“显著”就是要排除偶然随机性的因素，通过统计的方法来证明两者的不同是事实存在的，而不是由于波动性造成的巧合。

统计方法的步骤：

1. 用统计中的假设检验计算出相关的统计量，再分析测试结果

   > P值、置信区间

2. 常见的检验方法

   1. Z检验： 当评价指标为概率类指标时(比如转化率，注册率等等)，一般选用 Z 检验(在 A/B 测试 中有时又被称为比例检验(Proportion Test))来计算出相应的 P 值和置信区间。
   2. T检验：当评价指标为均值类指标时(比如人均使用时间，人均使用频率等等)，且在大样本量下 可以近似成正态分布时，一般选用 T 检验来计算相应的 P 值和置信区间。
   3. Bootstrapping: 当评价指标的分布比较复杂，在大样本量下也不能近似成正态分布时(比如 70% 用户的使 用时间，OEC 等)，一般采用 Bootstrapping 的方法



### 选择P值法还是置信区间法

**无论是 P 值法还是置信区间法，都可以用来分析 A/B 测试结果是否具有统计显著性。那么，在实际应用中该如何选择呢?两者有什么差别吗?**

其实，在大部分情况下这两种方法是通用的，只要选择一种就可以。但如果需要考虑实施变化后的收益和成本的关系时，我们就要选择置信区间法了。

要考虑收益和成本的关系时，除了满足结果在统计上是显著的(两组指标不相同，差值的置信区间不包括 0)还不够，更要让结果在业务上也是显著的(两组指标不仅要不相等，而且其差值 $δ >= δ_{\text{收支平衡}}$，并且差值的置信区间的范围都要比 $δ_{\text{收支平衡}}$ 大)。



# 常见问题

## 辛普森悖论

辛普森悖论出现的次数可能会很多，每次做AB测试结果的细分分析时，最好都要先检查下细分领域在两组的比例是否符合两组整体的比例，以确保实验结果的准确性。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321203708821.png" alt="image-20230321203708821" style="zoom:40%;" />



辛普森悖论：**指当多组数据内部组成分布不均匀时，从总体上比较多组数据和分别在各个细分领域中比较多组数据可能会得出相反的结论。**

从数学的语言来看，可以解释为： $\frac ab<\frac AB, \frac cd < \frac CD$,也可能会出现 $\frac {a+c}{b+d} >\frac{A+C}{B+D}$.



> 原因：存在我们尚未观测或不知道的潜在变量在对实验也有影响， 以及多组数据中各个细分领域的数据分布不均匀。辛普森悖论其实是理论上无法避免的，因为我们永远不知道哪些维度/特征也在发挥作用，而这些维度或许没被觉察到，或许没有被数据采集到。



在这个例子当中，其实是因为实验组和对照组虽然在总体上实现了我们在设计 实验时要求的样本量均分。但是在北京和上海这两个细分市场中却分布不均匀，没有实现样本量均分。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321203756636.png" alt="image-20230321203756636" style="zoom:30%;" />



> 为了避免辛普森悖论，除了要保证总体多组样本量要均匀，在各个细分领域上的多组样本分布也要均匀。



### 解决方法

1. 在分析测试结果前做好**合理性检验**，那出现辛普森悖论的几率就会大大减小

2. 如果我们在进行总体分析和细分分析时发现了辛普森悖论，最好的解决办法就是重新跑实验，看看两组在不同细分领域的分布不均会不会消失。

   > 如果分布不均的情况还是没有消失，那就说明这很可能不是偶然事件。需要检查是否是工程或者实验实施层面出现问题，并针对性地解决

3. 如果时间紧迫，没有时间重跑实验和检查问题原因，则就以细分领域的结 果为准，因为总体结果出现了辛普森悖论会变得不准确。

   > 但如果比较多个细分领域的结果，也可能又造成多重检验的问题

4. 因为维度根据分法不同可能有无穷多个，实践中我们能做的是**重点关注对我们有意义的维度，尽量减少它的影响**。



### 启示

#### 相关性、因果性

统计只告诉你相关性，而我们真正有兴趣的因果性，却必须依靠其他的考虑来确定。大到人类社会，小到产品功能，被研究的内容有着太多的维度，到底哪一个是因，哪一个是果不能简单论断。即使将能想到的维度按相关性强弱一一考虑，也只是减小出错的概率，更不能只挑选易得的、自己感兴趣的变量任意拿来研究并得出结论。

一个典型的社会学例子是疫情初始时，《经济学人》用新冠死亡率和“民主程度”统计，发现有正向相关性，然后断言“民主”有利于防疫，当然按照现在的情况，他们怕是不会再做一次“统计”。影响防疫的因素有太多，政治优先的《经济学人》只选择了自己感兴趣的那个维度，然后将其相关联，得到完全没有根据的结论。而在一个具体的产品中，普适型的数据（如粗暴的对比lOS和Android总体情况）也是没有多大参考意义的，最起码需要将用户、设备、场景等细分足够清楚去看，才能得到接近事实的结论。



#### 分层分类，定性定量

分层分类是将事实有关的内容区分的足够清楚明白，找到足够多的相关因素；而定性定量则是分层分类不可分割的下一步骤，数量与性质（权重）是不对等的，但是往往数量比性质更容易获得；因此在得到分层分类的数量数据后，需要斟酌个别分组的权重，以一定的系数去消除以分组资料基数差异所造成的影响，同时必须了解该情境是否存在其他潜在因素而综合考虑。



#### AB测试

工作中经常遇到的分组数据寻求结论场景就是AB测试，很多时候我们用不到5%的用户进行小规模的测试，发现效果很好，就直接一步跨到全量，最终效果可能并不如预期。我们可以通过三个方法避免这种情况的发生：

1. 首先是设计用户组时，尽量保证用户特征一致，并能代表产品的目标用户或者核心用户；
2. 其次当我们觉得某两个变量对试验结果都有影响时，需要将这两个变量放在同一层进行互斥试验，不要让一个变量的试验动态影响另一个变量的检验；
3. 最后分析结论时除了看大组的效果，也要针对一些细分用户进行效果回收，保证结果的可靠性。



> - 准确的用户分层在数据分析中是非常重要的，尤其是在免费产品当中，平均用户不仅不存在，而且是误导研发的因素之一，所以关键在于利用特征将用户进行合理划分。
>
> - 在一个具体的产品中，普适型的数据（如粗暴的对比IOS和Android总体情况）是没有多大参考意义的，一定要细分到具体设备、国家、获取渠道、消费能力等等再进行比对才有价值。
> - 斟酌个别分组的权重，以一定的系数去消除以分组资料基数差异所造成的影响，同时必需了解该情境是否存在其他潜在要因而综合考虑。

---



## 多重假设检验问题

多重检验问题：当存在多个假设检验且选择了其中最低的P值作为结果，这样会对P值和效应大小的估算出现偏差。

多重比较问题常出现：

1. 查看多个指标
2. 查看跨时间的P值
3. 查看受众细分群
4. 查看实验的多次迭代



多重检验问题，又叫多重测试问题或多重比较问题(Multiple Comparison Problem)，指的是当同时比较多个检验时，第一类错误率α就会增大，而结果的准确性就会受到影响这个问题。

### 多重检验为什么会是一个问题

要搞清楚多重检验为什么会是一个问题，我们还得先从第一类错误率α(又叫假阳性率，显著水平，是测试前的预设值，一般为 5%)说起。第一类错误率指的就是当事实上两组指标是相同的时候，假设检验推断出两组指标不同的概率，或者说由于偶然得到显著结果的概率。而且，它在统计上的约定俗成是 5%。

5% 看上去是个小概率事件，但是如果我们同时比较 20 个检验(测试)呢?你可以先思考一下，如果每个检验出现第一类错误的概率是 5%，那么在这 20 个检验中至少出现一个第一类错误的概率是多少呢?

要直接求出这个事件的概率不太容易，我们可以先求出这个事件发生情况的反面，也就是在这 20 个检验中完全没有出现第一类错误的概率，然后再用 100% 减去这个反面事件的概率。

这里我们用 P(A)来表示出现事件 A 的概率。P(每个检验出现第一类错误)=5%，那么P(每个检验不出现第一类错误) = (1-5%)=95%，所以 P(20 个检验中完全没有第一类错误)= 95% 的 20 次方。



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321204250201.png" alt="image-20230321204250201" style="zoom:40%;" />



这里的 P(至少出现一个第一类错误)的概率又叫做 **FWER** (Family-wise Error Rate)。

通过计算得出来的概率是 64%。这就意味着当同时比较 20 个检验时，在这 20 个结果中，至少出现一个第一类错误的概率是 64%。看看，这是不是个很大的概率了呢? 事实上，随着检验次数的增加，这个概率会越来越大.

<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321204322801.png" alt="image-20230321204322801" style="zoom:33%;" />

根据这个图我们可以 得出两个结论:

1. 随着检验次数的增加，FWER，也就是出现第一类错误的概率会显著升高。
2. 当α越小时，FWER 会越小，上升的速度也越慢。

第一个结论讲的就是多重检验带来的问题。第二个结论其实为我们提供了一种潜在的解决方法: **降低α**。

> 当我们同时比较多个检验时，就增加了得到第一类错误的概率(FWER)，这就变成了一个潜在的多重检验问题。



### 什么时候会遇到多重检验问题

#### 第一种形式，当 A/B 测试有不止一个实验组时

当我们想要改变不止一个变量且样本量充足时，我们可以不必等测试完一个变量后再去测试下一个，而是可以同时测试这些变量，把它们分在不同的实验组当中。

每个实验组只变化一个变量，在分析结果时分别用每个实验组和共同的对照组进行比较， 这种测试方法也叫做 **A/B/n 测试**。

> 比如我想要改变广告来提升其效果，那么想要改变的变 量包括内容、背景颜色、字体大小等等，这个时候我就要有相对应的 3 个实验组，然后把它们分别和对照组进行比较。这就相当于同时进行了 3 个检验，就会出现多重检验问题。



#### 第二种形式，当 A/B 测试有不止一个评价指标时

这个很好理解，因为我们分析测试结果，其实就是比较实验组和对照组的评价指标。如果 有多个评价指标的话，就会进行多次检验，产生多重检验问题。



#### 第三种形式，当你在分析 A/B 测试结果，按照不同的维度去做细分分析时

当我们分析测试结果时，根据业务需求，有时我们并不满足于只把实验组和对照组进行总体比较。 比如对于一个跨国公司来说，很多 A/B 测试会在全球多个国家同时进行，这时候如果我们 想要看 A/B 测试中的变化对于各个国家的具体影响时，就会以国家为维度来做细分的分析，会分别比较单个国家中的两组指标大小，那么此时分析每个国家的测试结果就是一个检验，多个国家则是多个检验。



#### 第四种形式，当 A/B 测试在进行过程中，你不断去查看实验结果时

因为当测试还在进行中，所以每次查看的测试都和上一次的不一样，每查看一次结果都算是一次检验，这样也会产生多重检验问题。

> 不要在 A/B 测试还在进行时就过早地去查看结果，一定要等样本量达到要求后再去计算结果



### 如何解决多重检验问题

四种形式的多重检验问题的解决方案：

鉴于多重检验问题的普遍性，在统计上有很多学者提出了自己的解决方法，大致分为两类:

1. 保持每个检验的 P 值不变，调整α。
2. 保持α不变，调整每个检验的 P 值。



用 P 值来判断假设检验的结果是否显著时，是用检验中计算出的 P 值和 α 进行比较的。当 $P  <α$时，我们才说结果显著。 所以，要么调整$α$，要么调整$ P$ 值。

- 降低$α$是一种解决办法，最常用的调整α的方法是**Bonferroni 校正**(Bonferroni Correction)，其实很简单，就是把$α$变成$α/n$。
  - Bonferroni 校正由于操作简单，在 A/B 测试的实践中十分流行，但是这种方法只是调整了 α，对于不同的 P 值都采取了一刀切的办法，所以显得有些保守，检测次数较少时还可以适用。
- 根据实践经验，在检测次数较大时(比如上百次，这种情况在 A/B 测试中出现的情况一般 是做不同维度的细分分析时，比如对于跨国公司来说，有时会有上百个 markets)， Bonferroni 校正会显著增加第二类错误率β，这时候一个比较好的解决办法就是去调整 P 值，常用的方法就是通过**控制 FDR(False Discovery Rate)**来实现。
  - 最常用的是**BH 法(Benjamini-Hochberg Procedure)**。
  - BH 法会考虑到每个 P 值的大小，然后做不同程度的调整。
    - 大致的调整方法就是把各个检验计算出的 P 值从小到大排序，
    - 然后根据排序来分别调整不同的α 值，
    - 最后再用调整后的 P 值和α进行比较。



---



## 学习效应

当我们想通过 A/B 测试检验非常明显的变化时，比如改变网站或者产品的交互界面和功能，那些网站或者产品的老客户往往适应了之前的交互界面和功能，而新的交互界面和功能对他们来说需要一段时间来适应和学习。所以往往老用户在学习适应阶段的行为会跟平 时有些不同，这就是学习效应。



### 学习效应在实践中的形式

老用户的两种学习适应期的反应：

- 第一种是积极的反应，一般也叫做新奇效应(Novelty Effect)，指的是老用户对于变化有很强的好奇心，愿意去尝试。
- 第二种是消极的反应，一般也叫做改变厌恶(Change Aversion)。指的是老用户对于变化比较困惑，甚至产生抵触心理。



### 如何检测

- 第一种方法是表征实验组的指标随着时间(以天为单位)的变化情况： 在没有学习效应的情况下，实验组的指标随着时间的变化是相对稳定的。 但是当有学习效应时，因为学习效应是短期的，长期来看慢慢会消退，那么实验组(有变化的组)的指标就会有一个随着时间慢慢变化的过程，直到稳定。
  - 如果是新奇效应，实验组的指标可能会由刚开始的迅速提升，到随着时间慢慢降低。
  - 如果是改变厌恶，实验组的指标可能会由刚开始的迅速降低，到随着时间慢慢回升。
  - ⚠️ 不需要每天都去比较实验，否则容易出现多重比较问题。只有达到样本量之后才可以去比较两组大小，分析测试结果。
- 第二种方法是只比较实验组和对照组中的新用户。
  - 学习效应是老用户为了学习适应新的变化产生的，所以对于新用户，也就是在实验期间才第一次登录的用户来说，并不存在“学习适应新的变化”这个问题，那么我们可以先在两组找出新用户(如果是随机分组的话，两组中新用户的比例应该是相似的)，然后只在两 组的新用户中分别计算我们的指标，最后再比较这两个指标。
  - 如果我们在新用户的比较中没有得出显著结果(在新用户样本量充足的情况下)，但是在 总体的比较中得出了显著结果，那就说明这个变化对于新用户没有影响，但是对于老用户有影响，那么大概率是出现了学习效应。

> 想真正排除学习效应的影响，得到准确的实验结果，还是要延长测试时间，等到实验组的学习效应消退再来比较两组的结果。





# 面试问题

## AB实验的结果在统计上显著，而在实际中却不显著，这是为什么？

1. 可能是AB实验中选取的**样本量过大**，导致实验样本和总体数据量差异很小，这样即使我们发现细微的差别，在统计意义上是显著的，在实际案例中也会变得不显著了

   > 💡 举个栗子，对应到我们的互联网产品实践当中，我们做了一个改动，APP的启动时间的优化了0.001秒，这个数字可能在统计学上对应的P值很小，也就是说统计学上是显著的，但是在实际中用户0.01秒的差异是感知不出来的。

2. 根据t检验统计量公式，如果样本量过大，只需要很小的区别就会造成显著

3. 可能出现了**第一类错误：将这种误导性的结论称为误报；而在统计学中，则称之为“Ⅰ 类错误”（即，当原假设正确时，您错误地拒绝了该假设）**

4. 只考虑了**短期的测试**，没有考虑**长期效应**（短期新奇效应）

5. 可能在测试期间更改了流量分配策略

6. 过早的停止了测试，或者在测试过程中多次观测实验



> 测试完成后做假设检验时构造置信区间受到以下三个关键因素影响：
>
> - 测试样本量： 唯一可控的因素
> - 总体标准偏差
> - 显著性水平





---

## 在AB实验中发现我们选取的指标在统计意义上都不显著，那如何判断这个实验的收益

**弄清楚统计显著性和实际显著性，有时候在一些app上即使是没有达到统计显著性，例如Google某些功能上1%，2%的实际显著性也是非常高的。**



1. **拆分到天**：将指标拆分成每一天去观察，如果**在实验中每一天实验组都高于对照组，即使在统计意义上不显著，我们也认为在这个观测周期内，实验组的关键指标的表现是优于对照组的**，然后再比较这个效益的**增幅与我们预期增幅对比**，若达到预期，最终也可以得出优化可以上线的结论
2. **拆分人群**：通过假设实验对每个人效果都一致的，然而这个前提往往不成立，**例如三四线人群和一二线人群对同一促销活动感知不一样**。在整体实验组和对照组结果不显著前提下，可以将实验组和对照组中的人群按特征（年龄、城市）单独拆分出来进行分析，在某些特征维度上可能结果是显著的。



**如果实际效果不错，但是AB测试不显著？（比如指标提升了10%但不显著）**

- 指标方差过大&样本量太少——对策：增大样本量或者随机区组试验
- 第二类错误



---



## 实验组按照5%流量随机分流的依据是什么

实验组按照5%流量随机分流的依据可能是为了在进行 A/B 测试时，对实验组和对照组的流量进行平衡。

在使用 A/B 测试时，随机分流是很重要的。这意味着将参与者随机分配到实验组和对照组，以确保两组人群的差异不会影响测试结果。如果不使用随机分流，那么两组人群可能会存在明显的差异，这会对测试结果产生影响。

根据 5% 的流量随机分流的依据，意味着将参与者随机分配到实验组和对照组，使得两组人群的大小比例相差不超过 5%。这种方法可以帮助确保两组人群之间的差异较小。



---



## 算法部门上线了新的推荐算法，在ab-test中败给了老算法，让你找出其中的原因，需要说出具体思路和框架

在发现新的推荐算法在 AB-test 中败给了老算法后，我会采取以下思路和框架来寻找原因：

1. **分析 AB-test 的设置情况**：首先，我会查看 AB-test 的设置情况或者是实施中出现了一些问题，要检查包括**样本选择是否合理、AB-test 时间是否足够长、控制组和实验组的分布情况是否均衡等**。 如果发现 AB-test 设置存在问题，则可能是 AB-test 的结果并不具有可信度，需要进一步调整 AB-test 的设置。
2. 分析推荐算法的工作原理：其次，我会深入了解新的推荐算法的工作原理，**并与老算法进行比较。 如果发现新算法在某些方面的表现不如老算法**，则可能是新算法的工作原理存在问题，需要进一步调整和优化。
3. 分析数据的特征：再者，我会分析 AB-test 中使用的**数据的特征**，包括**数据的质量、数据的分布情况等**。 如果发现数据存在某些特殊的特征，则可能会对新算法的表现产生影响，需要进一步分析。（数据对算法的质量是有很重要的影响）
4. **分析用户的行为数据**：最后，我会分析 AB-test 中，可能算法中的一些改动让用户明显察觉到不同，导致部分用户体验下降；可以分析 A 算法和 B 算法在处理用户行为数据时的表现情况，看看哪个算法能够更好地根据用户行为数据进行推荐。
5. 其他因素分析：在上述步骤中，如果还没有找到新算法较差的原因，则可能需要考虑其他因素。 例如，可能是**新算法的实现存在bug问题**，或者是**新算法的调参过程存在问题**。 这时，需要进一步分析新算法的实现情况和调参过程，并尝试纠正可能存在的问题。
6. 总结并形成建议：在完成上述步骤后，我会总结所有可能影响新算法表现的因素，并根据分析结果形成建议。 例如，可以提出修改新算法的工作原理、调整 AB-test 的设置、优化数据的质量等建议，以期望在下一次 AB-test 中取得更好的结果。



---



## 在没有做AB实验的前提下，如何评估策略迭代的优劣

B实验的本质是为了解决「因果问题」，在没有做AB实验的情况下，可以通过：DID（双重拆分法）、传递熵、因果森林等方式进行替代。

不过总体来说，AB实验仍然是处理因果问题最简单、最直接的方式。

---



## 如何判断实验组、对照组的某个指标是否有显著差异

1. 在实验开始前，对实验组和对照组先进行数据指标的监测，实验前两组指标没有显著差异（AA实验，检验两组实验人群不存在明显差异）
2. 实验结束，观测实验后的结果，根据假设检验原理设置显著性水平，在该水平下判断两组的指标是否有显著差异
3. 若实验前两组就存在差异，可采用**DID（双重差分）**的方法，查看两组的指标差距在设定显著性水平下实验前后是否有显著差异。



---

## 实际显著性vs统计显著性

什么时候需要考虑实际显著性？

> 如果是**需要改变现有的状态且改变有一定成本时**需要是要考虑实际显著性的，但是 对于探索性质的且改变成本不高的情况下可以不考虑.



---



## 实验有效性

对内部有效性的威胁：实验还没有推广到其他人群或时间段情况下的实验结果的正确性。

1. 违法了个体处理稳定性假设
2. 幸存者偏差：只分析了特定的用户
3. 意向性分析
4. 样本比率不匹配(Sample ratio mismatch, SRM)



对外部有效性的威胁：对照试验可以沿不同维度，如人群、时间的推广程度

基于时间的外部有效性威胁

1. 初始效应： 新功能的适应需要时间
2. 新奇效应：无法持续的效应，新功能可能吸引用户尝试，但是若功能无用，用户重复使用次数会减少

---



## 线上对照试验的关键点

关于线上对照实验的几个关键主题：

- **一个想法的价值很难被预估。**在这个案例中，一个价值超过每年1亿美金的简单的产品改动被耽搁了好几个月。
- **小改动可以有大影响。**一个工程师几天的工作就能带来每年1亿美金的回报。当然这样极端的投资回报率（return-on-investment, ROI）也很罕见。
- **有很大影响的实验是少见的。**必应每年运行上万个实验，但这种小改动实现大增长的案例几年才出一个。
- **运行实验的启动成本要低。**必应的工程师可以使用微软的实验平台ExP，来便利地科学评估产品改动。
- **综合评估标准（overall evaluation criterion，OEC）必须清晰。**在这个案例中，营收是OEC的一个关键组成，但仅营收本身不足以成为一个OEC。以营收为唯一指标可能导致网站满是广告而伤害用户体验。必应使用的OEC权衡了营收指标和用户体验指标，包括人均会话数（用户是否放弃使用或者活跃度增加）和其他一些成分。关键宗旨是即使营收大幅增长，用户体验指标也不能显著下降。
- 两组实验执行上的时间要具有一致性
- 两组数据在各个维度上特征分布的一致性
- 实验分组流量分配上药均匀
- 实验周期中也要避免外部因素的影响，尽量在平稳时期进行，减少外部因素的干扰；有时候为了保证实验效果的置信，防止小流量分布不均匀，可以在试验过程中，**逐步增大流量分配**，同时监控关键指标的数据走势，从而得到置信的结论；

---



## AB实验的原理和本质

1. 原理：来源于假设检验。有两个同质的样本组，对其中一个组做出某种改动，然后来观测这个改动对于我们关注的指标是否有显著的影响

   - 原假设：这项改动不会对我们关注的指标有显著的影响

   - 如果在做完试验后发现P值足够小，则推翻原假设，证明改动有影响

2. 同质样本组的对照实验



---

## AB实验中辛普森悖论

1. 在某种条件下，我们关注的两组数据，如果分别讨论，则这两组数据都会满足某种同样的性质，而当我们把两个子数据集合并再观察整体，却会得出截然相反的结论
2. **原因**：我们把“值”与“量”两个维度的数据合并成了“值”， 即我们划分数据集时，并没有对流量进行一个合理的分割，导致我们所选取的实验组并不具有一定的代表性
3. **影响**：在互联网案例中，我们使用1%的用户数据去跑实验，结论是改动效果好，结果上线后，全量用户结果却是新版本带来的用户体验下降
4. **避免方法**：保证对样本量进行一个合理的分配，并保证我们选取的样本量具有相似的特征，都能代表总体的特征



---



## 第一列错误和第二列错误的概念？实际含义？

1. 第一类错误：原假设是正确的，却拒绝了原假设。
   - 原假设$μ1=μ2$,原假设是正确的，意思是$μ1=μ2$，即这个功能的改变不能带来收益，但是误判，以为能带来收益
2. 第二类错误：原假设是错误的，却没有拒绝原假设。
   - 举例：原假设$μ1=μ2$,原假设是错误的，意思是$μ1≠μ2$，即这个功能的改变能带来收益，但是误判，以为不能带来收益

---



## 第一类错误和第二类错误哪个更不能接受

在实际工作中，第一类错误时我们更加不能接受的。换一句更加直白的话说，就是**我们宁愿砍掉4个好的产品，也不让一个坏的产品上线**。因为一个坏的产品上线，对用户体验影响很大。



---



## AB实验的开设流程

AB实验经常运用在活动策略是否有效的问题上，进行实验的步骤是：实验的流程：

**确定目标和假设->确定指标->确定实验单位->计算样本量->实施测试->分析实验结果**



AB实验的流程:

1. 与相关PM沟通确定验证点：确定实验所要验证的功能、改动点在哪里，确定目标和假设

2. 分析师确认在实验中需要观测的一些核心指标

3. 确认实验的样本流量：一般为20w → 确定实验单位, 以及计算样本量

4. 实施测试：

   - 发邮件和相关PM以及开发同学确认可以开启实验，确认实验配置

   - 在发版正式实验前，一般会通过小流量开启一段时间的灰度实验，来验证我们的实验并不会造成一些特别极端的影响

   - 正式发版，一般执行1周左右时间

5. 在实验分析平台，整理实验数据，产出实验报告 → 分析实验结果



📢 注意点：

- 其中确定指标中比较关键的是要确定评价指标和护栏指标
  - 评价指标就是驱动公司实现核心价值的指标，要具有可归因性、可测量性、敏感性和稳定性；
  - 护栏指标也就是辅助指标
- 确定实验单位有从用户层面、访问层面和页面层面进行考虑的情况:
  - 用户层面适用于易被用户察觉的变化实验，访问和页面层面适用于不易被用户察觉的变化实验；
  - 从用户层面到页面层面实验粒度越来越细，累计的样本量也越来越多
- 计算样本量，需要预先确认以下数值：显著性水平、功效、实验组和对照组的综合方差以及期望的最小差值
  - 实验组和对照组数据量最好均分，非均分的时候只有相对较小的组达到最小样本量，实验结果才可能显著
  - 并不是说实验组越大越好，因为**瓶颈是在样本量较小的对照组上**，所以实验组和对照组的样本量最好相同 (避免出现辛普森悖论？）
- 分析测试结果的时候要注意辛普森悖论等问题，而且要保证样本达到足够的量、检验是否在正常的波动范围内

---



## 在AB实验中发现关注的核心指标有一个显著的提升（显著正向），那么这个优化一定能上线吗？

**不一定**

> 从性能上看，一方面的优化可能会导致另一方面的劣化，要多综合评估所有方面的一些指标变动，同时对收益和损失进行评估，再来确认优化是否能上线



---



## 若每次改动都要进行一次AB实验测试，这样成本不会变高吗？

要考虑成本投入的问题。

1. 如果是验证一个小按钮或小改动，我们可以在界面上设置一个开关，用户可以通过开关的形式自行决定采用哪种方式，最终我们可以通过开关的相关指标去判断用户对哪种形式有更大的倾向性
2. 或者可进行一些用户调研，如访谈或问卷形式来收集



**AB测试缺陷：**

1. 无法测试新的体验
2. 无法告诉你是否遗漏内容
3. 无法对时间跨度较长的实验



在测试新的体验时，AB测试的用途就没那么大了。

还有一些时间跨度要求比较大的实验，也不适合进行AB测试。例如酒店订购app的推荐，大家住酒店的频率不会像是日常活动，可能这次订了酒店，即使用户将该APP推荐给其他人，他人用到这个APP的时间可能要过去一段时间。

AB测试无法确切告诉你，你是否遗漏了什么东西。



---



## 流程、原理



### AB测试的一般流程

1. 确定改变点
2. 确定核心指标
3. 计算实验所需的最小样本流量与试验周期
4. 设计流量分割策略
5. 灰度实验
6. 显著性检验



### 流程细分

1. 确定改变点 保证单一因素原则，即一次实验不能糅合多个影响因素

2. 确定核心指标

   - 绝对值指标(较少)

   - 比率类指标(常见)

     > 转化率、点击率、复购率等

3. 计算实验所需的最小样本流量与试验周期

   > 通过公式或工具计算

4. 设计流量分割策略

   - 辛普森悖论：细分结果与总计结果相悖

   - 除改变点外，其他影响结果的变量应保持一致

   - 分流分层原理

     1. 分流 一份数据分流为多份数据

     1. 分层 一份数据分层到多个数据层(各个数据层正交，互不影响)

---

## 谈谈对AB测试的理解

AB测试本质上是假设检验。

而在AB测试中，在 A/B 测试的语境下，零假设指的是实验组和对照组的指标是相同的，备择假设指的是实验组和对照组的指标是不同的。

原假设：两个组的指标没有差异，备择假设：两个组的指标存在显著差异。

AB测试有助于快速迭代产品功能，促进业务的持续增长。



---



## AB测试如何确定目标和假设

> 在 A/B 测试中确定目标和假设的重要性。A/B 测试是和业务紧密相关的，但我们往往会忽视业务中的目标，把注意力过多地放在选取评价指标上。
>
> 在我看来，这就是本末倒置，就像一个不知道终点在哪里却一直在奔跑的运动员，如果能先 明确终点，朝着终点的方向努力，会更快地取得成功。

1. 分析问题，确定想要达到的结果
2. 提出解决业务问题的大致方案
3. 从大致的解决方案中提取出具体的假设



假设要避免：

1. 拒绝模糊、太空
2. 拒绝太主观推断，要基于事实、经验等
3. 拒绝问题的原因、结果不明确
4. 拒绝定性问题



<img src="/Users/siheng_huang/Library/Application%20Support/typora-user-images/image-20230321202403503.png" alt="image-20230321202403503" style="zoom:50%;" />



---

## AB测试中的方差很大，如何解决

AB实验场景下，**如果一个指标的方差较大表示它的波动较大，那么实验组和对照组的显著差异可能是因为方差较大即随机波动较大**。

- 解决方法有：

  - PSM方法

    - PSM倾向值匹配方法（Propensity Score Matching)：观测性研究有时无法人为控制干扰因素，因此可能会导致因果推断的偏差。

    - 常规的解决思路是

      尽量模拟随机试验

      , 这样实验组与对照组在结果变量上的差异就可归因与实验条件的改变而非干扰因素或协变量施加的影响。PSM基于反事实因果模理论发展而成，属于因果推断的一种，相当于人为去造一个理想的实验环境

      - 倾向得分匹配，此时对照组和实验组对象间除了处理外，没有其他不同

  - CUPED方差缩减方法（Controlled-experiment Using Pre-Experiment Data）：

    - 先分层计算后汇总，举个例子，我们计算对照组和实验组的用户平均使用时长，可以分别按照城市划分，先计算每个城市的用户平均使用时长，然后再按照权重(各城市实验用户)计算总的。(前提是城市这个特征与用户平均使用时长高度相关)



机器学习场景下，特征的方差反而越大越好，因为如果一个特征方差为0，那么其实这个特征对于模型来说没有什么意义，所以特征方差大对于模型的训练才是有帮助的。



---

## 如何确定分流样本不倾斜

- AA test，或者说分流之后先空跑一段时间看是否显著
- 检查各个协变量指标，要么和关注指标无相关性，要么在实验组和对照组分布相近。
- 或许可以用所有特征对分组结果建立逻辑回归模型，如果变量不显著，可以认为在该变量上分组均匀。（比如男女，不需要1：1，只要不显著即可）
- 使用随机分流：这是最常用的方法，即通过随机数生成器将用户随机分配到实验组或对照组。这样可以确保样本分布在两个组中是平均的，从而减少偏差。

---

## 做留存率的ABTest，选择什么检验？

A留存率是指用户在使用产品之后还会继续使用产品的比率。在进行ABTest时，如果要比较两组用户的留存率，可以使用**卡方检验**来进行比较。

**卡方检验是一种用于比较两组分类数据的差异的统计检验方法**，它可以用来检验两组数据是否有显著差异。在进行留存率的ABTest时，可以将两组用户的留存情况分成“留存”和“不留存”两类，然后使用卡方检验来比较两组用户的留存率的差异是否显著。

在使用卡方检验时，需要注意的是，卡方检验的前提是**两组数据的分布独立**，如果两组数据的分布不独立，则卡方检验的结果可能会失真。此外，**卡方检验的结果受样本大小的影响较大，当样本数量较小时，结果的可信度较低**。



---

## AB实验的样本选择时，应该注意什么

在选择AB实验的样本时，应该注意以下几点：

1. 可比性、均衡性：AB实验的两个样本应该尽可能相似均衡，两个样本在重要的特征方面应该尽可能相似，以便能够更好地对比结果。
2. 样本数量：AB实验的样本数量应该足够大，以便能够得到较为准确的结果。
3. 随机性：AB实验的样本应该用随机的方式选择，以便能够更好地控制其他可能影响结果的因素。
4. 独立性：两组样本应该组间样本应该相互独立，不能产生影响，否则容易破坏SUTVA假设
5. 排除外部干扰因素：AB实验的样本应该尽可能排除外部的干扰因素，以便能够得到更准确的结果。

注意，AB实验的样本选择是影响实验结果的重要因素。 因此，在选择样本时应该认真考虑以上几点，以便能够得到较为可靠的结果。
